{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:03:41.045675Z",
     "start_time": "2019-05-20T22:03:41.040155Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras_preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:04:33.334746Z",
     "start_time": "2019-05-20T22:04:31.042826Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:06:52.559136Z",
     "start_time": "2019-05-20T22:06:52.424365Z"
    }
   },
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df.Product\n",
    "complaints = df['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:09:55.026775Z",
     "start_time": "2019-05-20T22:09:41.736404Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:11:47.672316Z",
     "start_time": "2019-05-20T22:11:47.648552Z"
    }
   },
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product)\n",
    "\n",
    "product_one_hot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:19:21.801497Z",
     "start_time": "2019-05-20T22:19:21.176807Z"
    }
   },
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_one_hot, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:20:02.342883Z",
     "start_time": "2019-05-20T22:20:02.309008Z"
    }
   },
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:23:37.695587Z",
     "start_time": "2019-05-20T22:23:37.201493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/michaelmoravetz/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:25:03.266365Z",
     "start_time": "2019-05-20T22:25:03.106703Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:26:19.306871Z",
     "start_time": "2019-05-20T22:25:29.539405Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/michaelmoravetz/anaconda3/envs/learn-env/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 148us/step - loss: 1.9500 - acc: 0.1733 - val_loss: 1.9414 - val_acc: 0.1710\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.9296 - acc: 0.2085 - val_loss: 1.9256 - val_acc: 0.2110\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.9131 - acc: 0.2367 - val_loss: 1.9099 - val_acc: 0.2300\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.8968 - acc: 0.2496 - val_loss: 1.8934 - val_acc: 0.2440\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.8795 - acc: 0.2668 - val_loss: 1.8750 - val_acc: 0.2630\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.8600 - acc: 0.2819 - val_loss: 1.8541 - val_acc: 0.2850\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.8376 - acc: 0.3035 - val_loss: 1.8299 - val_acc: 0.3010\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8111 - acc: 0.3184 - val_loss: 1.8015 - val_acc: 0.3230\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.7797 - acc: 0.3396 - val_loss: 1.7674 - val_acc: 0.3350\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7432 - acc: 0.3585 - val_loss: 1.7289 - val_acc: 0.3560\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.7022 - acc: 0.3843 - val_loss: 1.6856 - val_acc: 0.3840\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.6558 - acc: 0.4144 - val_loss: 1.6373 - val_acc: 0.4110\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.6041 - acc: 0.4403 - val_loss: 1.5843 - val_acc: 0.4720\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.5488 - acc: 0.4840 - val_loss: 1.5290 - val_acc: 0.5080\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.4904 - acc: 0.5192 - val_loss: 1.4712 - val_acc: 0.5340\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.4291 - acc: 0.5617 - val_loss: 1.4124 - val_acc: 0.5610\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.3679 - acc: 0.5937 - val_loss: 1.3541 - val_acc: 0.5920\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.3078 - acc: 0.6165 - val_loss: 1.2978 - val_acc: 0.6200\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.2501 - acc: 0.6368 - val_loss: 1.2458 - val_acc: 0.6370\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.1956 - acc: 0.6548 - val_loss: 1.1942 - val_acc: 0.6610\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1448 - acc: 0.6708 - val_loss: 1.1482 - val_acc: 0.6740\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0979 - acc: 0.6812 - val_loss: 1.1062 - val_acc: 0.6740\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0540 - acc: 0.6895 - val_loss: 1.0651 - val_acc: 0.6940\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0140 - acc: 0.6992 - val_loss: 1.0293 - val_acc: 0.7000\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9775 - acc: 0.7057 - val_loss: 1.0006 - val_acc: 0.6970\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9449 - acc: 0.7128 - val_loss: 0.9691 - val_acc: 0.7190\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9142 - acc: 0.7215 - val_loss: 0.9428 - val_acc: 0.7140\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8865 - acc: 0.7251 - val_loss: 0.9181 - val_acc: 0.7210\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8613 - acc: 0.7300 - val_loss: 0.8972 - val_acc: 0.7230\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8384 - acc: 0.7355 - val_loss: 0.8764 - val_acc: 0.7280\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8174 - acc: 0.7401 - val_loss: 0.8599 - val_acc: 0.7300\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.7982 - acc: 0.7431 - val_loss: 0.8435 - val_acc: 0.7340\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.7802 - acc: 0.7459 - val_loss: 0.8292 - val_acc: 0.7370\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.7636 - acc: 0.7504 - val_loss: 0.8142 - val_acc: 0.7420\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.7484 - acc: 0.7521 - val_loss: 0.8021 - val_acc: 0.7400\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7342 - acc: 0.7559 - val_loss: 0.7908 - val_acc: 0.7450\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.7208 - acc: 0.7589 - val_loss: 0.7800 - val_acc: 0.7450\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.7083 - acc: 0.7629 - val_loss: 0.7703 - val_acc: 0.7490\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.6969 - acc: 0.7671 - val_loss: 0.7609 - val_acc: 0.7470\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.6858 - acc: 0.7700 - val_loss: 0.7522 - val_acc: 0.7450\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.6751 - acc: 0.7736 - val_loss: 0.7450 - val_acc: 0.7490\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6653 - acc: 0.7744 - val_loss: 0.7367 - val_acc: 0.7470\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.6564 - acc: 0.7785 - val_loss: 0.7318 - val_acc: 0.7530\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.6471 - acc: 0.7828 - val_loss: 0.7236 - val_acc: 0.7610\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.6388 - acc: 0.7824 - val_loss: 0.7173 - val_acc: 0.7500\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.6304 - acc: 0.7859 - val_loss: 0.7134 - val_acc: 0.7640\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.6227 - acc: 0.7891 - val_loss: 0.7065 - val_acc: 0.7660\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.6154 - acc: 0.7925 - val_loss: 0.7025 - val_acc: 0.7530\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.6080 - acc: 0.7928 - val_loss: 0.6966 - val_acc: 0.7590\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.6018 - acc: 0.7955 - val_loss: 0.6919 - val_acc: 0.7510\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.5945 - acc: 0.7964 - val_loss: 0.6902 - val_acc: 0.7690\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5885 - acc: 0.7988 - val_loss: 0.6849 - val_acc: 0.7740\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.5821 - acc: 0.8015 - val_loss: 0.6815 - val_acc: 0.7590\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5758 - acc: 0.8044 - val_loss: 0.6780 - val_acc: 0.7630\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.5702 - acc: 0.8060 - val_loss: 0.6738 - val_acc: 0.7680\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.5644 - acc: 0.8069 - val_loss: 0.6705 - val_acc: 0.7660\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.5584 - acc: 0.8088 - val_loss: 0.6680 - val_acc: 0.7710\n",
      "Epoch 58/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5536 - acc: 0.8137 - val_loss: 0.6633 - val_acc: 0.7720\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.5481 - acc: 0.8135 - val_loss: 0.6611 - val_acc: 0.7780\n",
      "Epoch 60/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5428 - acc: 0.8169 - val_loss: 0.6580 - val_acc: 0.7710\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5379 - acc: 0.8180 - val_loss: 0.6579 - val_acc: 0.7660\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.5327 - acc: 0.8192 - val_loss: 0.6537 - val_acc: 0.7720\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5277 - acc: 0.8237 - val_loss: 0.6514 - val_acc: 0.7840\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5232 - acc: 0.8240 - val_loss: 0.6494 - val_acc: 0.7810\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5184 - acc: 0.8259 - val_loss: 0.6470 - val_acc: 0.7770\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.5138 - acc: 0.8284 - val_loss: 0.6454 - val_acc: 0.7780\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.5095 - acc: 0.8303 - val_loss: 0.6456 - val_acc: 0.7830\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.5053 - acc: 0.8300 - val_loss: 0.6406 - val_acc: 0.7850\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.5005 - acc: 0.8331 - val_loss: 0.6399 - val_acc: 0.7840\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4966 - acc: 0.8333 - val_loss: 0.6369 - val_acc: 0.7860\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4919 - acc: 0.8347 - val_loss: 0.6363 - val_acc: 0.7820\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4880 - acc: 0.8375 - val_loss: 0.6345 - val_acc: 0.7810\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4838 - acc: 0.8392 - val_loss: 0.6335 - val_acc: 0.7830\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4801 - acc: 0.8420 - val_loss: 0.6324 - val_acc: 0.7800\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4760 - acc: 0.8435 - val_loss: 0.6316 - val_acc: 0.7870\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4721 - acc: 0.8437 - val_loss: 0.6291 - val_acc: 0.7870\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4682 - acc: 0.8460 - val_loss: 0.6287 - val_acc: 0.7800\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.4646 - acc: 0.8460 - val_loss: 0.6275 - val_acc: 0.7810\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4609 - acc: 0.8495 - val_loss: 0.6263 - val_acc: 0.7830\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4571 - acc: 0.8496 - val_loss: 0.6248 - val_acc: 0.7880\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4535 - acc: 0.8501 - val_loss: 0.6245 - val_acc: 0.7820\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4497 - acc: 0.8515 - val_loss: 0.6223 - val_acc: 0.7870\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.4464 - acc: 0.8533 - val_loss: 0.6218 - val_acc: 0.7810\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4430 - acc: 0.8544 - val_loss: 0.6213 - val_acc: 0.7810\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.4393 - acc: 0.8548 - val_loss: 0.6213 - val_acc: 0.7860\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.4358 - acc: 0.8579 - val_loss: 0.6206 - val_acc: 0.7840\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4325 - acc: 0.8595 - val_loss: 0.6201 - val_acc: 0.7820\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4295 - acc: 0.8593 - val_loss: 0.6238 - val_acc: 0.7820\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.4263 - acc: 0.8611 - val_loss: 0.6186 - val_acc: 0.7850\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.4228 - acc: 0.8615 - val_loss: 0.6178 - val_acc: 0.7820\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.4195 - acc: 0.8637 - val_loss: 0.6163 - val_acc: 0.7830\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4162 - acc: 0.8643 - val_loss: 0.6174 - val_acc: 0.7830\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.4132 - acc: 0.8651 - val_loss: 0.6155 - val_acc: 0.7840\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4097 - acc: 0.8673 - val_loss: 0.6154 - val_acc: 0.7850\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.4066 - acc: 0.8681 - val_loss: 0.6160 - val_acc: 0.7870\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.4036 - acc: 0.8700 - val_loss: 0.6156 - val_acc: 0.7820\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.4003 - acc: 0.8692 - val_loss: 0.6148 - val_acc: 0.7830\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.3973 - acc: 0.8720 - val_loss: 0.6165 - val_acc: 0.7820\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.3947 - acc: 0.8715 - val_loss: 0.6161 - val_acc: 0.7850\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.3917 - acc: 0.8717 - val_loss: 0.6155 - val_acc: 0.7820\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3891 - acc: 0.8752 - val_loss: 0.6146 - val_acc: 0.7850\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.3860 - acc: 0.8765 - val_loss: 0.6141 - val_acc: 0.7830\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.3831 - acc: 0.8768 - val_loss: 0.6128 - val_acc: 0.7850\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.3798 - acc: 0.8768 - val_loss: 0.6147 - val_acc: 0.7860\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3775 - acc: 0.8789 - val_loss: 0.6137 - val_acc: 0.7850\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.3748 - acc: 0.8803 - val_loss: 0.6144 - val_acc: 0.7820\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.3721 - acc: 0.8801 - val_loss: 0.6128 - val_acc: 0.7840\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.3686 - acc: 0.8825 - val_loss: 0.6177 - val_acc: 0.7760\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.3668 - acc: 0.8827 - val_loss: 0.6130 - val_acc: 0.7870\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.3638 - acc: 0.8847 - val_loss: 0.6137 - val_acc: 0.7810\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.3609 - acc: 0.8845 - val_loss: 0.6129 - val_acc: 0.7810\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.3586 - acc: 0.8863 - val_loss: 0.6142 - val_acc: 0.7830\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.3559 - acc: 0.8868 - val_loss: 0.6125 - val_acc: 0.7820\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.3532 - acc: 0.8887 - val_loss: 0.6142 - val_acc: 0.7800\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.3506 - acc: 0.8913 - val_loss: 0.6155 - val_acc: 0.7780\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.3484 - acc: 0.8909 - val_loss: 0.6138 - val_acc: 0.7840\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.3456 - acc: 0.8927 - val_loss: 0.6164 - val_acc: 0.7850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.3433 - acc: 0.8939 - val_loss: 0.6185 - val_acc: 0.7790\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.3407 - acc: 0.8935 - val_loss: 0.6150 - val_acc: 0.7830\n",
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.3381 - acc: 0.8977 - val_loss: 0.6152 - val_acc: 0.7810\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:32:20.649742Z",
     "start_time": "2019-05-20T22:32:20.613587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:32:41.140641Z",
     "start_time": "2019-05-20T22:32:40.462821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 88us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:32:53.610490Z",
     "start_time": "2019-05-20T22:32:53.472575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 86us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:32:55.586550Z",
     "start_time": "2019-05-20T22:32:55.564972Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.33490909845034283, 0.8972]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:32:57.456629Z",
     "start_time": "2019-05-20T22:32:57.448998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6235104262034098, 0.7853333331743876]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:33:13.098768Z",
     "start_time": "2019-05-20T22:33:12.023005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:34:17.447674Z",
     "start_time": "2019-05-20T22:34:16.843873Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:35:33.069885Z",
     "start_time": "2019-05-20T22:35:03.605488Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 2s 206us/step - loss: 1.9624 - acc: 0.1139 - val_loss: 1.9428 - val_acc: 0.1220\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.9389 - acc: 0.1491 - val_loss: 1.9275 - val_acc: 0.1750\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.9241 - acc: 0.1947 - val_loss: 1.9147 - val_acc: 0.2160\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.9103 - acc: 0.2241 - val_loss: 1.9009 - val_acc: 0.2520\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8947 - acc: 0.2591 - val_loss: 1.8836 - val_acc: 0.2770\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.8755 - acc: 0.2868 - val_loss: 1.8629 - val_acc: 0.3030\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.8535 - acc: 0.3097 - val_loss: 1.8397 - val_acc: 0.3100\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.8291 - acc: 0.3325 - val_loss: 1.8145 - val_acc: 0.3190\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.8022 - acc: 0.3536 - val_loss: 1.7866 - val_acc: 0.3370\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.7718 - acc: 0.3728 - val_loss: 1.7546 - val_acc: 0.3770\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.7376 - acc: 0.3980 - val_loss: 1.7186 - val_acc: 0.4050\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.6999 - acc: 0.4209 - val_loss: 1.6793 - val_acc: 0.4340\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.6589 - acc: 0.4497 - val_loss: 1.6374 - val_acc: 0.4510\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.6149 - acc: 0.4631 - val_loss: 1.5931 - val_acc: 0.4950\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.5684 - acc: 0.4879 - val_loss: 1.5479 - val_acc: 0.5000\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.5211 - acc: 0.5068 - val_loss: 1.5013 - val_acc: 0.5190\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.4733 - acc: 0.5281 - val_loss: 1.4559 - val_acc: 0.5360\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.4257 - acc: 0.5501 - val_loss: 1.4108 - val_acc: 0.5520\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.3792 - acc: 0.5648 - val_loss: 1.3672 - val_acc: 0.5700\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.3334 - acc: 0.5828 - val_loss: 1.3258 - val_acc: 0.5800\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2895 - acc: 0.5988 - val_loss: 1.2835 - val_acc: 0.5940\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2467 - acc: 0.6143 - val_loss: 1.2432 - val_acc: 0.6110\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.2058 - acc: 0.6329 - val_loss: 1.2048 - val_acc: 0.6300\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.1663 - acc: 0.6479 - val_loss: 1.1693 - val_acc: 0.6400\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.1291 - acc: 0.6583 - val_loss: 1.1351 - val_acc: 0.6630\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0935 - acc: 0.6721 - val_loss: 1.1042 - val_acc: 0.6700\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 1.0600 - acc: 0.6816 - val_loss: 1.0717 - val_acc: 0.6870\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0276 - acc: 0.6947 - val_loss: 1.0433 - val_acc: 0.6860\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9978 - acc: 0.7041 - val_loss: 1.0165 - val_acc: 0.6940\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9695 - acc: 0.7091 - val_loss: 0.9916 - val_acc: 0.6950\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9429 - acc: 0.7175 - val_loss: 0.9661 - val_acc: 0.7120\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9175 - acc: 0.7247 - val_loss: 0.9444 - val_acc: 0.7120\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8940 - acc: 0.7321 - val_loss: 0.9224 - val_acc: 0.7240\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8720 - acc: 0.7353 - val_loss: 0.9037 - val_acc: 0.7250\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8514 - acc: 0.7385 - val_loss: 0.8856 - val_acc: 0.7300\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8320 - acc: 0.7444 - val_loss: 0.8682 - val_acc: 0.7340\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8135 - acc: 0.7460 - val_loss: 0.8527 - val_acc: 0.7360\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7964 - acc: 0.7497 - val_loss: 0.8383 - val_acc: 0.7390\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.7801 - acc: 0.7549 - val_loss: 0.8243 - val_acc: 0.7450\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.7646 - acc: 0.7591 - val_loss: 0.8121 - val_acc: 0.7400\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7506 - acc: 0.7604 - val_loss: 0.7993 - val_acc: 0.7440\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.7373 - acc: 0.7627 - val_loss: 0.7876 - val_acc: 0.7460\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.7244 - acc: 0.7679 - val_loss: 0.7775 - val_acc: 0.7470\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.7131 - acc: 0.7696 - val_loss: 0.7673 - val_acc: 0.7490\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.7019 - acc: 0.7712 - val_loss: 0.7583 - val_acc: 0.7480\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.6910 - acc: 0.7739 - val_loss: 0.7510 - val_acc: 0.7520\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.6816 - acc: 0.7772 - val_loss: 0.7427 - val_acc: 0.7520\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.6720 - acc: 0.7772 - val_loss: 0.7362 - val_acc: 0.7630\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.6626 - acc: 0.7844 - val_loss: 0.7275 - val_acc: 0.7590\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.6544 - acc: 0.7827 - val_loss: 0.7228 - val_acc: 0.7590\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.6463 - acc: 0.7843 - val_loss: 0.7167 - val_acc: 0.7620\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.6384 - acc: 0.7859 - val_loss: 0.7100 - val_acc: 0.7620\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.6308 - acc: 0.7884 - val_loss: 0.7050 - val_acc: 0.7590\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.6238 - acc: 0.7927 - val_loss: 0.6996 - val_acc: 0.7630\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.6170 - acc: 0.7925 - val_loss: 0.6942 - val_acc: 0.7640\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.6101 - acc: 0.7949 - val_loss: 0.6919 - val_acc: 0.7690\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.6035 - acc: 0.7969 - val_loss: 0.6858 - val_acc: 0.7700\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.5976 - acc: 0.7965 - val_loss: 0.6821 - val_acc: 0.7640\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.5910 - acc: 0.7993 - val_loss: 0.6817 - val_acc: 0.7680\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.5853 - acc: 0.8016 - val_loss: 0.6735 - val_acc: 0.7720\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:36:13.666438Z",
     "start_time": "2019-05-20T22:36:13.265983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 53us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:36:15.971982Z",
     "start_time": "2019-05-20T22:36:15.831038Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 89us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:36:17.976300Z",
     "start_time": "2019-05-20T22:36:17.968649Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.579950529829661, 0.8035999999682109]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:36:20.550877Z",
     "start_time": "2019-05-20T22:36:20.544331Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6770344813664754, 0.758666666507721]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:37:54.911326Z",
     "start_time": "2019-05-20T22:37:03.469669Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 1s 116us/step - loss: 2.5896 - acc: 0.1988 - val_loss: 2.5757 - val_acc: 0.1950\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.5641 - acc: 0.2120 - val_loss: 2.5549 - val_acc: 0.2090\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.5433 - acc: 0.2172 - val_loss: 2.5350 - val_acc: 0.2180\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 2.5219 - acc: 0.2271 - val_loss: 2.5138 - val_acc: 0.2260\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 2.4986 - acc: 0.2412 - val_loss: 2.4904 - val_acc: 0.2340\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 2.4726 - acc: 0.2511 - val_loss: 2.4644 - val_acc: 0.2670\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 2.4442 - acc: 0.2737 - val_loss: 2.4359 - val_acc: 0.2720\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 2.4131 - acc: 0.2861 - val_loss: 2.4052 - val_acc: 0.2930\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.3799 - acc: 0.3049 - val_loss: 2.3723 - val_acc: 0.3090\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 2.3447 - acc: 0.3288 - val_loss: 2.3369 - val_acc: 0.3320\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 2.3067 - acc: 0.3536 - val_loss: 2.2992 - val_acc: 0.3640\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.2663 - acc: 0.3771 - val_loss: 2.2580 - val_acc: 0.3830\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 2.2238 - acc: 0.4037 - val_loss: 2.2169 - val_acc: 0.4080\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 2.1812 - acc: 0.4252 - val_loss: 2.1760 - val_acc: 0.4460\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 2.1392 - acc: 0.4535 - val_loss: 2.1362 - val_acc: 0.4650\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 2.0976 - acc: 0.4748 - val_loss: 2.0957 - val_acc: 0.4800\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 2.0563 - acc: 0.4917 - val_loss: 2.0573 - val_acc: 0.4920\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 2.0156 - acc: 0.5099 - val_loss: 2.0181 - val_acc: 0.5020\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.9754 - acc: 0.5284 - val_loss: 1.9792 - val_acc: 0.5140\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.9355 - acc: 0.5433 - val_loss: 1.9414 - val_acc: 0.5380\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.8967 - acc: 0.5627 - val_loss: 1.9041 - val_acc: 0.5610\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.8585 - acc: 0.5805 - val_loss: 1.8686 - val_acc: 0.5770\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.8216 - acc: 0.5955 - val_loss: 1.8331 - val_acc: 0.6010\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.7862 - acc: 0.6107 - val_loss: 1.8001 - val_acc: 0.6090\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7517 - acc: 0.6255 - val_loss: 1.7665 - val_acc: 0.6150\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.7193 - acc: 0.6411 - val_loss: 1.7362 - val_acc: 0.6320\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6878 - acc: 0.6536 - val_loss: 1.7092 - val_acc: 0.6380\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.6584 - acc: 0.6632 - val_loss: 1.6802 - val_acc: 0.6520\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.6299 - acc: 0.6717 - val_loss: 1.6526 - val_acc: 0.6600\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.6024 - acc: 0.6835 - val_loss: 1.6271 - val_acc: 0.6730\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.5761 - acc: 0.6911 - val_loss: 1.6027 - val_acc: 0.6830\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.5515 - acc: 0.6968 - val_loss: 1.5789 - val_acc: 0.6940\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.5276 - acc: 0.7011 - val_loss: 1.5576 - val_acc: 0.6930\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.5053 - acc: 0.7087 - val_loss: 1.5377 - val_acc: 0.6990\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.4842 - acc: 0.7127 - val_loss: 1.5177 - val_acc: 0.7060\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.4641 - acc: 0.7172 - val_loss: 1.4993 - val_acc: 0.7140\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.4450 - acc: 0.7213 - val_loss: 1.4842 - val_acc: 0.7150\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.4271 - acc: 0.7235 - val_loss: 1.4662 - val_acc: 0.7210\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.4102 - acc: 0.7307 - val_loss: 1.4523 - val_acc: 0.7250\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.3937 - acc: 0.7349 - val_loss: 1.4371 - val_acc: 0.7280\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.3781 - acc: 0.7407 - val_loss: 1.4224 - val_acc: 0.7340\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3632 - acc: 0.7424 - val_loss: 1.4084 - val_acc: 0.7380\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3490 - acc: 0.7459 - val_loss: 1.3970 - val_acc: 0.7340\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3357 - acc: 0.7520 - val_loss: 1.3846 - val_acc: 0.7400\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3228 - acc: 0.7528 - val_loss: 1.3741 - val_acc: 0.7410\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3100 - acc: 0.7577 - val_loss: 1.3654 - val_acc: 0.7450\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2986 - acc: 0.7580 - val_loss: 1.3528 - val_acc: 0.7480\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2869 - acc: 0.7617 - val_loss: 1.3439 - val_acc: 0.7500\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2761 - acc: 0.7645 - val_loss: 1.3343 - val_acc: 0.7490\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.2655 - acc: 0.7677 - val_loss: 1.3250 - val_acc: 0.7510\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2555 - acc: 0.7693 - val_loss: 1.3179 - val_acc: 0.7530\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2459 - acc: 0.7731 - val_loss: 1.3084 - val_acc: 0.7520\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.2361 - acc: 0.7759 - val_loss: 1.3000 - val_acc: 0.7520\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2267 - acc: 0.7776 - val_loss: 1.2943 - val_acc: 0.7560\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.2180 - acc: 0.7797 - val_loss: 1.2856 - val_acc: 0.7560\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2090 - acc: 0.7833 - val_loss: 1.2778 - val_acc: 0.7570\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.2007 - acc: 0.7860 - val_loss: 1.2723 - val_acc: 0.7600\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1921 - acc: 0.7855 - val_loss: 1.2660 - val_acc: 0.7580\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1843 - acc: 0.7885 - val_loss: 1.2579 - val_acc: 0.7650\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1764 - acc: 0.7912 - val_loss: 1.2536 - val_acc: 0.7600\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1682 - acc: 0.7924 - val_loss: 1.2485 - val_acc: 0.7660\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1611 - acc: 0.7923 - val_loss: 1.2423 - val_acc: 0.7690\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1542 - acc: 0.7955 - val_loss: 1.2378 - val_acc: 0.7660\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1467 - acc: 0.7972 - val_loss: 1.2288 - val_acc: 0.7680\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1393 - acc: 0.8001 - val_loss: 1.2257 - val_acc: 0.7660\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1323 - acc: 0.7997 - val_loss: 1.2190 - val_acc: 0.7700\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1258 - acc: 0.8031 - val_loss: 1.2141 - val_acc: 0.7740\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.1191 - acc: 0.8039 - val_loss: 1.2086 - val_acc: 0.7720\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.1125 - acc: 0.8063 - val_loss: 1.2040 - val_acc: 0.7710\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1061 - acc: 0.8089 - val_loss: 1.1992 - val_acc: 0.7710\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0999 - acc: 0.8096 - val_loss: 1.1964 - val_acc: 0.7760\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0936 - acc: 0.8128 - val_loss: 1.1899 - val_acc: 0.7740\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0871 - acc: 0.8133 - val_loss: 1.1874 - val_acc: 0.7780\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0815 - acc: 0.8143 - val_loss: 1.1833 - val_acc: 0.7760\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0755 - acc: 0.8167 - val_loss: 1.1789 - val_acc: 0.7740\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0696 - acc: 0.8175 - val_loss: 1.1754 - val_acc: 0.7680\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0639 - acc: 0.8192 - val_loss: 1.1705 - val_acc: 0.7690\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0580 - acc: 0.8203 - val_loss: 1.1681 - val_acc: 0.7720\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0524 - acc: 0.8213 - val_loss: 1.1619 - val_acc: 0.7800\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0471 - acc: 0.8239 - val_loss: 1.1572 - val_acc: 0.7770\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0413 - acc: 0.8231 - val_loss: 1.1541 - val_acc: 0.7740\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0363 - acc: 0.8268 - val_loss: 1.1517 - val_acc: 0.7750\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0306 - acc: 0.8261 - val_loss: 1.1473 - val_acc: 0.7830\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0258 - acc: 0.8271 - val_loss: 1.1433 - val_acc: 0.7810\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0205 - acc: 0.8297 - val_loss: 1.1427 - val_acc: 0.7780\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0151 - acc: 0.8305 - val_loss: 1.1373 - val_acc: 0.7770\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0101 - acc: 0.8305 - val_loss: 1.1334 - val_acc: 0.7820\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0050 - acc: 0.8347 - val_loss: 1.1294 - val_acc: 0.7830\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.0002 - acc: 0.8347 - val_loss: 1.1269 - val_acc: 0.7770\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9950 - acc: 0.8343 - val_loss: 1.1241 - val_acc: 0.7760\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9906 - acc: 0.8356 - val_loss: 1.1216 - val_acc: 0.7780\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9857 - acc: 0.8385 - val_loss: 1.1174 - val_acc: 0.7810\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9811 - acc: 0.8388 - val_loss: 1.1155 - val_acc: 0.7790\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9763 - acc: 0.8427 - val_loss: 1.1113 - val_acc: 0.7780\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9713 - acc: 0.8443 - val_loss: 1.1115 - val_acc: 0.7790\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9672 - acc: 0.8440 - val_loss: 1.1050 - val_acc: 0.7850\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9626 - acc: 0.8467 - val_loss: 1.1029 - val_acc: 0.7830\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9582 - acc: 0.8483 - val_loss: 1.0998 - val_acc: 0.7810\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9535 - acc: 0.8487 - val_loss: 1.0983 - val_acc: 0.7840\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9491 - acc: 0.8488 - val_loss: 1.0944 - val_acc: 0.7810\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9447 - acc: 0.8504 - val_loss: 1.0911 - val_acc: 0.7840\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9404 - acc: 0.8547 - val_loss: 1.0891 - val_acc: 0.7850\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9361 - acc: 0.8543 - val_loss: 1.0869 - val_acc: 0.7850\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9318 - acc: 0.8568 - val_loss: 1.0848 - val_acc: 0.7830\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9277 - acc: 0.8561 - val_loss: 1.0822 - val_acc: 0.7840\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9233 - acc: 0.8571 - val_loss: 1.0795 - val_acc: 0.7850\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9195 - acc: 0.8601 - val_loss: 1.0777 - val_acc: 0.7850\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9157 - acc: 0.8620 - val_loss: 1.0745 - val_acc: 0.7870\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9117 - acc: 0.8620 - val_loss: 1.0727 - val_acc: 0.7860\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9072 - acc: 0.8613 - val_loss: 1.0709 - val_acc: 0.7910\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9038 - acc: 0.8624 - val_loss: 1.0682 - val_acc: 0.7880\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8997 - acc: 0.8641 - val_loss: 1.0666 - val_acc: 0.7860\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8957 - acc: 0.8640 - val_loss: 1.0638 - val_acc: 0.7870\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8919 - acc: 0.8656 - val_loss: 1.0617 - val_acc: 0.7830\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8879 - acc: 0.8683 - val_loss: 1.0603 - val_acc: 0.7850\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8840 - acc: 0.8699 - val_loss: 1.0592 - val_acc: 0.7880\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8805 - acc: 0.8691 - val_loss: 1.0554 - val_acc: 0.7920\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8772 - acc: 0.8704 - val_loss: 1.0528 - val_acc: 0.7870\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8731 - acc: 0.8708 - val_loss: 1.0514 - val_acc: 0.7890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8695 - acc: 0.8735 - val_loss: 1.0501 - val_acc: 0.7860\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:39:25.193833Z",
     "start_time": "2019-05-20T22:39:25.159216Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:39:27.816036Z",
     "start_time": "2019-05-20T22:39:27.126576Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:41:15.586405Z",
     "start_time": "2019-05-20T22:40:22.973426Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 2s 288us/step - loss: 15.9810 - acc: 0.1523 - val_loss: 15.5814 - val_acc: 0.1770\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 15.2184 - acc: 0.1999 - val_loss: 14.8298 - val_acc: 0.2140\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 14.4757 - acc: 0.2311 - val_loss: 14.0975 - val_acc: 0.2290\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 13.7530 - acc: 0.2436 - val_loss: 13.3856 - val_acc: 0.2490\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 13.0489 - acc: 0.2648 - val_loss: 12.6910 - val_acc: 0.2590\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 12.3603 - acc: 0.2907 - val_loss: 12.0123 - val_acc: 0.3020\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 11.6886 - acc: 0.3313 - val_loss: 11.3506 - val_acc: 0.3410\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 11.0371 - acc: 0.3719 - val_loss: 10.7107 - val_acc: 0.3770\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 10.4085 - acc: 0.4145 - val_loss: 10.0955 - val_acc: 0.4360\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 9.8039 - acc: 0.4500 - val_loss: 9.5037 - val_acc: 0.4570\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 9.2226 - acc: 0.4817 - val_loss: 8.9358 - val_acc: 0.4850\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 8.6642 - acc: 0.5125 - val_loss: 8.3907 - val_acc: 0.4790\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 8.1293 - acc: 0.5269 - val_loss: 7.8679 - val_acc: 0.5080\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 7.6175 - acc: 0.5477 - val_loss: 7.3683 - val_acc: 0.5290\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 7.1287 - acc: 0.5609 - val_loss: 6.8930 - val_acc: 0.5470\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 6.6639 - acc: 0.5755 - val_loss: 6.4403 - val_acc: 0.5610\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 6.2229 - acc: 0.5904 - val_loss: 6.0119 - val_acc: 0.5810\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 5.8058 - acc: 0.6035 - val_loss: 5.6071 - val_acc: 0.5980\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 5.4124 - acc: 0.6116 - val_loss: 5.2272 - val_acc: 0.6090\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 5.0427 - acc: 0.6216 - val_loss: 4.8692 - val_acc: 0.6190\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 4.6954 - acc: 0.6316 - val_loss: 4.5334 - val_acc: 0.6270\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 4.3708 - acc: 0.6421 - val_loss: 4.2210 - val_acc: 0.6370\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 4.0689 - acc: 0.6493 - val_loss: 3.9297 - val_acc: 0.6370\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 3.7885 - acc: 0.6499 - val_loss: 3.6618 - val_acc: 0.6430\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 3.5311 - acc: 0.6593 - val_loss: 3.4143 - val_acc: 0.6570\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 3.2956 - acc: 0.6603 - val_loss: 3.1907 - val_acc: 0.6490\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 3.0817 - acc: 0.6639 - val_loss: 2.9865 - val_acc: 0.6610\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 2.8892 - acc: 0.6659 - val_loss: 2.8045 - val_acc: 0.6690\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 2.7176 - acc: 0.6677 - val_loss: 2.6444 - val_acc: 0.6740\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 2.5671 - acc: 0.6716 - val_loss: 2.5038 - val_acc: 0.6640\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 2.4370 - acc: 0.6705 - val_loss: 2.3819 - val_acc: 0.6750\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.3264 - acc: 0.6735 - val_loss: 2.2819 - val_acc: 0.6680\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 2.2354 - acc: 0.6723 - val_loss: 2.1989 - val_acc: 0.6710\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 2.1619 - acc: 0.6699 - val_loss: 2.1346 - val_acc: 0.6790\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 2.1055 - acc: 0.6725 - val_loss: 2.0877 - val_acc: 0.6790\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 2.0634 - acc: 0.6721 - val_loss: 2.0482 - val_acc: 0.6810\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 2.0319 - acc: 0.6728 - val_loss: 2.0220 - val_acc: 0.6760\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 2.0076 - acc: 0.6736 - val_loss: 1.9964 - val_acc: 0.6760\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.9863 - acc: 0.6749 - val_loss: 1.9776 - val_acc: 0.6790\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.9672 - acc: 0.6732 - val_loss: 1.9610 - val_acc: 0.6830\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.9495 - acc: 0.6767 - val_loss: 1.9391 - val_acc: 0.6800\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.9327 - acc: 0.6753 - val_loss: 1.9233 - val_acc: 0.6770\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.9167 - acc: 0.6741 - val_loss: 1.9077 - val_acc: 0.6820\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.9019 - acc: 0.6759 - val_loss: 1.8957 - val_acc: 0.6840\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.8875 - acc: 0.6781 - val_loss: 1.8791 - val_acc: 0.6820\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.8736 - acc: 0.6765 - val_loss: 1.8662 - val_acc: 0.6840\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.8603 - acc: 0.6789 - val_loss: 1.8570 - val_acc: 0.6850\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.8470 - acc: 0.6784 - val_loss: 1.8387 - val_acc: 0.6890\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.8342 - acc: 0.6784 - val_loss: 1.8256 - val_acc: 0.6850\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.8218 - acc: 0.6795 - val_loss: 1.8154 - val_acc: 0.6890\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.8100 - acc: 0.6803 - val_loss: 1.8000 - val_acc: 0.6860\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.7981 - acc: 0.6800 - val_loss: 1.7932 - val_acc: 0.6950\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.7871 - acc: 0.6801 - val_loss: 1.7825 - val_acc: 0.6950\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.7765 - acc: 0.6817 - val_loss: 1.7672 - val_acc: 0.7030\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.7653 - acc: 0.6825 - val_loss: 1.7582 - val_acc: 0.7050\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.7545 - acc: 0.6841 - val_loss: 1.7512 - val_acc: 0.7000\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.7443 - acc: 0.6841 - val_loss: 1.7358 - val_acc: 0.7000\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.7341 - acc: 0.6823 - val_loss: 1.7264 - val_acc: 0.6990\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.7239 - acc: 0.6835 - val_loss: 1.7145 - val_acc: 0.6970\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.7144 - acc: 0.6864 - val_loss: 1.7047 - val_acc: 0.6980\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.7045 - acc: 0.6867 - val_loss: 1.6975 - val_acc: 0.6970\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.6954 - acc: 0.6857 - val_loss: 1.6862 - val_acc: 0.7000\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.6858 - acc: 0.6863 - val_loss: 1.6785 - val_acc: 0.6960\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6770 - acc: 0.6871 - val_loss: 1.6667 - val_acc: 0.7020\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6681 - acc: 0.6871 - val_loss: 1.6596 - val_acc: 0.7030\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.6597 - acc: 0.6887 - val_loss: 1.6526 - val_acc: 0.7040\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6512 - acc: 0.6888 - val_loss: 1.6487 - val_acc: 0.7130\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.6435 - acc: 0.6876 - val_loss: 1.6329 - val_acc: 0.7010\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.6344 - acc: 0.6892 - val_loss: 1.6277 - val_acc: 0.6950\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6267 - acc: 0.6904 - val_loss: 1.6199 - val_acc: 0.7150\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.6188 - acc: 0.6905 - val_loss: 1.6112 - val_acc: 0.7070\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.6103 - acc: 0.6899 - val_loss: 1.6012 - val_acc: 0.7100\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.6022 - acc: 0.6920 - val_loss: 1.5978 - val_acc: 0.7090\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5951 - acc: 0.6907 - val_loss: 1.5870 - val_acc: 0.7160\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.5871 - acc: 0.6920 - val_loss: 1.5790 - val_acc: 0.7120\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.5797 - acc: 0.6917 - val_loss: 1.5763 - val_acc: 0.7090\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.5728 - acc: 0.6921 - val_loss: 1.5662 - val_acc: 0.7160\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.5648 - acc: 0.6933 - val_loss: 1.5581 - val_acc: 0.7120\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.5580 - acc: 0.6933 - val_loss: 1.5563 - val_acc: 0.7070\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.5507 - acc: 0.6923 - val_loss: 1.5436 - val_acc: 0.7120\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.5435 - acc: 0.6932 - val_loss: 1.5371 - val_acc: 0.7070\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5369 - acc: 0.6943 - val_loss: 1.5293 - val_acc: 0.7080\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.5296 - acc: 0.6945 - val_loss: 1.5221 - val_acc: 0.7150\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5227 - acc: 0.6956 - val_loss: 1.5181 - val_acc: 0.7110\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5158 - acc: 0.6955 - val_loss: 1.5078 - val_acc: 0.7160\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5087 - acc: 0.6957 - val_loss: 1.5051 - val_acc: 0.7110\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.5019 - acc: 0.6960 - val_loss: 1.4964 - val_acc: 0.7180\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4956 - acc: 0.6963 - val_loss: 1.4892 - val_acc: 0.7210\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4892 - acc: 0.6977 - val_loss: 1.4825 - val_acc: 0.7140\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.4827 - acc: 0.6981 - val_loss: 1.4775 - val_acc: 0.7190\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4759 - acc: 0.6964 - val_loss: 1.4686 - val_acc: 0.7200\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4700 - acc: 0.6980 - val_loss: 1.4632 - val_acc: 0.7120\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4634 - acc: 0.6983 - val_loss: 1.4566 - val_acc: 0.7190\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4574 - acc: 0.6983 - val_loss: 1.4515 - val_acc: 0.7210\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4512 - acc: 0.6991 - val_loss: 1.4487 - val_acc: 0.7100\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4451 - acc: 0.6996 - val_loss: 1.4377 - val_acc: 0.7200\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.4388 - acc: 0.7004 - val_loss: 1.4430 - val_acc: 0.7200\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.4331 - acc: 0.7009 - val_loss: 1.4259 - val_acc: 0.7150\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.4265 - acc: 0.7013 - val_loss: 1.4193 - val_acc: 0.7150\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4211 - acc: 0.7025 - val_loss: 1.4176 - val_acc: 0.7190\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.4154 - acc: 0.7039 - val_loss: 1.4117 - val_acc: 0.7220\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.4092 - acc: 0.7037 - val_loss: 1.4063 - val_acc: 0.7190\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4035 - acc: 0.7045 - val_loss: 1.4006 - val_acc: 0.7230\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3979 - acc: 0.7044 - val_loss: 1.3966 - val_acc: 0.7240\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3923 - acc: 0.7043 - val_loss: 1.3859 - val_acc: 0.7220\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.3866 - acc: 0.7061 - val_loss: 1.3842 - val_acc: 0.7250\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.3812 - acc: 0.7079 - val_loss: 1.3783 - val_acc: 0.7230\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.3757 - acc: 0.7076 - val_loss: 1.3721 - val_acc: 0.7260\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.3704 - acc: 0.7089 - val_loss: 1.3675 - val_acc: 0.7250\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3650 - acc: 0.7079 - val_loss: 1.3618 - val_acc: 0.7240\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3597 - acc: 0.7075 - val_loss: 1.3565 - val_acc: 0.7260\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.3543 - acc: 0.7097 - val_loss: 1.3526 - val_acc: 0.7180\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.3486 - acc: 0.7092 - val_loss: 1.3440 - val_acc: 0.7240\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.3437 - acc: 0.7092 - val_loss: 1.3427 - val_acc: 0.7250\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3383 - acc: 0.7096 - val_loss: 1.3325 - val_acc: 0.7270\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3335 - acc: 0.7113 - val_loss: 1.3282 - val_acc: 0.7260\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.3286 - acc: 0.7093 - val_loss: 1.3273 - val_acc: 0.7270\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.3236 - acc: 0.7112 - val_loss: 1.3201 - val_acc: 0.7290\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.3184 - acc: 0.7121 - val_loss: 1.3162 - val_acc: 0.7260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.3137 - acc: 0.7144 - val_loss: 1.3081 - val_acc: 0.7250\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:41:35.944433Z",
     "start_time": "2019-05-20T22:41:35.447950Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:49:10.998972Z",
     "start_time": "2019-05-20T22:42:00.694109Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 1s 135us/step - loss: 15.9687 - acc: 0.1623 - val_loss: 15.5438 - val_acc: 0.1900\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 15.1879 - acc: 0.2299 - val_loss: 14.7821 - val_acc: 0.2390\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 14.4364 - acc: 0.2844 - val_loss: 14.0426 - val_acc: 0.2910\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 13.7053 - acc: 0.3236 - val_loss: 13.3228 - val_acc: 0.3310\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 12.9937 - acc: 0.3524 - val_loss: 12.6229 - val_acc: 0.3500\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 12.3022 - acc: 0.3729 - val_loss: 11.9435 - val_acc: 0.3730\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 11.6321 - acc: 0.3904 - val_loss: 11.2865 - val_acc: 0.3820\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 10.9845 - acc: 0.4169 - val_loss: 10.6519 - val_acc: 0.4070\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 10.3594 - acc: 0.4293 - val_loss: 10.0397 - val_acc: 0.4300\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 9.7567 - acc: 0.4495 - val_loss: 9.4507 - val_acc: 0.4400\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 9.1774 - acc: 0.4632 - val_loss: 8.8842 - val_acc: 0.4520\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 8.6220 - acc: 0.4811 - val_loss: 8.3425 - val_acc: 0.4660\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 8.0897 - acc: 0.4955 - val_loss: 7.8238 - val_acc: 0.5000\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.5802 - acc: 0.5137 - val_loss: 7.3265 - val_acc: 0.5080\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 7.0945 - acc: 0.5328 - val_loss: 6.8532 - val_acc: 0.5070\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 6.6322 - acc: 0.5355 - val_loss: 6.4045 - val_acc: 0.5380\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 6.1934 - acc: 0.5540 - val_loss: 5.9783 - val_acc: 0.5540\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 5.7778 - acc: 0.5645 - val_loss: 5.5751 - val_acc: 0.5590\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 5.3851 - acc: 0.5799 - val_loss: 5.1957 - val_acc: 0.5640\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 5.0153 - acc: 0.5869 - val_loss: 4.8378 - val_acc: 0.5660\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 4.6685 - acc: 0.5944 - val_loss: 4.5032 - val_acc: 0.6070\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 4.3446 - acc: 0.6057 - val_loss: 4.1902 - val_acc: 0.6130\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 4.0428 - acc: 0.6139 - val_loss: 3.9002 - val_acc: 0.6090\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 3.7630 - acc: 0.6179 - val_loss: 3.6317 - val_acc: 0.6280\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 3.5045 - acc: 0.6245 - val_loss: 3.3839 - val_acc: 0.6330\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 3.2673 - acc: 0.6281 - val_loss: 3.1575 - val_acc: 0.6370\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 3.0514 - acc: 0.6336 - val_loss: 2.9521 - val_acc: 0.6320\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 2.8570 - acc: 0.6351 - val_loss: 2.7714 - val_acc: 0.6520\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 2.6833 - acc: 0.6391 - val_loss: 2.6054 - val_acc: 0.6410\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 2.5302 - acc: 0.6401 - val_loss: 2.4622 - val_acc: 0.6480\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 2.3978 - acc: 0.6415 - val_loss: 2.3413 - val_acc: 0.6540\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 2.2855 - acc: 0.6412 - val_loss: 2.2387 - val_acc: 0.6450\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 2.1929 - acc: 0.6419 - val_loss: 2.1542 - val_acc: 0.6560\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 2.1189 - acc: 0.6429 - val_loss: 2.0928 - val_acc: 0.6610\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 2.0629 - acc: 0.6447 - val_loss: 2.0409 - val_acc: 0.6600\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 2.0224 - acc: 0.6428 - val_loss: 2.0085 - val_acc: 0.6630\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.9931 - acc: 0.6460 - val_loss: 1.9817 - val_acc: 0.6610\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.9700 - acc: 0.6473 - val_loss: 1.9601 - val_acc: 0.6620\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.9506 - acc: 0.6460 - val_loss: 1.9410 - val_acc: 0.6590\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.9324 - acc: 0.6488 - val_loss: 1.9239 - val_acc: 0.6670\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.9163 - acc: 0.6513 - val_loss: 1.9069 - val_acc: 0.6630\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.9007 - acc: 0.6535 - val_loss: 1.8910 - val_acc: 0.6670\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.8866 - acc: 0.6532 - val_loss: 1.8781 - val_acc: 0.6730\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.8725 - acc: 0.6549 - val_loss: 1.8652 - val_acc: 0.6720\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.8594 - acc: 0.6563 - val_loss: 1.8517 - val_acc: 0.6680\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.8473 - acc: 0.6571 - val_loss: 1.8357 - val_acc: 0.6670\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.8344 - acc: 0.6581 - val_loss: 1.8249 - val_acc: 0.6730\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.8227 - acc: 0.6616 - val_loss: 1.8153 - val_acc: 0.6750\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.8117 - acc: 0.6617 - val_loss: 1.8017 - val_acc: 0.6690\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.7998 - acc: 0.6617 - val_loss: 1.7929 - val_acc: 0.6740\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.7892 - acc: 0.6665 - val_loss: 1.7782 - val_acc: 0.6750\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.7790 - acc: 0.6648 - val_loss: 1.7718 - val_acc: 0.6850\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.7686 - acc: 0.6660 - val_loss: 1.7576 - val_acc: 0.6770\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.7584 - acc: 0.6668 - val_loss: 1.7482 - val_acc: 0.6720\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.7484 - acc: 0.6689 - val_loss: 1.7412 - val_acc: 0.6820\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.7388 - acc: 0.6699 - val_loss: 1.7276 - val_acc: 0.6810\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.7287 - acc: 0.6713 - val_loss: 1.7218 - val_acc: 0.6900\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.7197 - acc: 0.6712 - val_loss: 1.7128 - val_acc: 0.6900\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.7105 - acc: 0.6735 - val_loss: 1.7082 - val_acc: 0.6900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.7022 - acc: 0.6733 - val_loss: 1.6925 - val_acc: 0.6920\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.6924 - acc: 0.6749 - val_loss: 1.6814 - val_acc: 0.6900\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.6834 - acc: 0.6756 - val_loss: 1.6726 - val_acc: 0.6850\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.6746 - acc: 0.6776 - val_loss: 1.6621 - val_acc: 0.6860\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.6660 - acc: 0.6777 - val_loss: 1.6613 - val_acc: 0.6940\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.6579 - acc: 0.6775 - val_loss: 1.6488 - val_acc: 0.6870\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.6500 - acc: 0.6796 - val_loss: 1.6385 - val_acc: 0.6910\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.6418 - acc: 0.6805 - val_loss: 1.6293 - val_acc: 0.6890\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.6333 - acc: 0.6819 - val_loss: 1.6212 - val_acc: 0.6910\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.6257 - acc: 0.6821 - val_loss: 1.6151 - val_acc: 0.6950\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.6178 - acc: 0.6823 - val_loss: 1.6143 - val_acc: 0.6930\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.6102 - acc: 0.6831 - val_loss: 1.6003 - val_acc: 0.6930\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.6025 - acc: 0.6848 - val_loss: 1.5928 - val_acc: 0.6980\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.5948 - acc: 0.6840 - val_loss: 1.5846 - val_acc: 0.7010\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.5869 - acc: 0.6849 - val_loss: 1.5761 - val_acc: 0.6980\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.5799 - acc: 0.6868 - val_loss: 1.5703 - val_acc: 0.7000\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.5727 - acc: 0.6884 - val_loss: 1.5588 - val_acc: 0.6980\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.5652 - acc: 0.6865 - val_loss: 1.5546 - val_acc: 0.7020\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.5586 - acc: 0.6871 - val_loss: 1.5472 - val_acc: 0.7020\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.5511 - acc: 0.6905 - val_loss: 1.5405 - val_acc: 0.7000\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.5447 - acc: 0.6915 - val_loss: 1.5350 - val_acc: 0.7080\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.5373 - acc: 0.6903 - val_loss: 1.5266 - val_acc: 0.7050\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.5308 - acc: 0.6900 - val_loss: 1.5191 - val_acc: 0.7020\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.5238 - acc: 0.6921 - val_loss: 1.5105 - val_acc: 0.7070\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.5168 - acc: 0.6933 - val_loss: 1.5035 - val_acc: 0.7120\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.5099 - acc: 0.6909 - val_loss: 1.4989 - val_acc: 0.7100\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.5036 - acc: 0.6955 - val_loss: 1.4929 - val_acc: 0.7130\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.4973 - acc: 0.6945 - val_loss: 1.4832 - val_acc: 0.7140\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.4904 - acc: 0.6957 - val_loss: 1.4767 - val_acc: 0.7150\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.4840 - acc: 0.6952 - val_loss: 1.4847 - val_acc: 0.7120\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.4784 - acc: 0.6971 - val_loss: 1.4728 - val_acc: 0.7140\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.4719 - acc: 0.6963 - val_loss: 1.4606 - val_acc: 0.7180\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.4654 - acc: 0.6981 - val_loss: 1.4523 - val_acc: 0.7170\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.4593 - acc: 0.6968 - val_loss: 1.4474 - val_acc: 0.7210\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.4528 - acc: 0.6999 - val_loss: 1.4403 - val_acc: 0.7170\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.4471 - acc: 0.6985 - val_loss: 1.4388 - val_acc: 0.7210\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.4413 - acc: 0.7007 - val_loss: 1.4265 - val_acc: 0.7220\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.4346 - acc: 0.6999 - val_loss: 1.4249 - val_acc: 0.7200\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.4295 - acc: 0.7004 - val_loss: 1.4177 - val_acc: 0.7220\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.4234 - acc: 0.7003 - val_loss: 1.4112 - val_acc: 0.7260\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.4177 - acc: 0.7015 - val_loss: 1.4102 - val_acc: 0.7180\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.4122 - acc: 0.7023 - val_loss: 1.4032 - val_acc: 0.7210\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.4065 - acc: 0.7043 - val_loss: 1.3923 - val_acc: 0.7260\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.4007 - acc: 0.7031 - val_loss: 1.3896 - val_acc: 0.7230\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3952 - acc: 0.7023 - val_loss: 1.3851 - val_acc: 0.7240\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.3901 - acc: 0.7044 - val_loss: 1.3794 - val_acc: 0.7170\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.3844 - acc: 0.7059 - val_loss: 1.3705 - val_acc: 0.7340\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3793 - acc: 0.7040 - val_loss: 1.3702 - val_acc: 0.7200\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3746 - acc: 0.7071 - val_loss: 1.3627 - val_acc: 0.7230\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.3686 - acc: 0.7065 - val_loss: 1.3544 - val_acc: 0.7300\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.3629 - acc: 0.7067 - val_loss: 1.3528 - val_acc: 0.7270\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.3579 - acc: 0.7071 - val_loss: 1.3425 - val_acc: 0.7350\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.3525 - acc: 0.7096 - val_loss: 1.3431 - val_acc: 0.7300\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3478 - acc: 0.7089 - val_loss: 1.3345 - val_acc: 0.7330\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.3430 - acc: 0.7099 - val_loss: 1.3301 - val_acc: 0.7310\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3378 - acc: 0.7107 - val_loss: 1.3305 - val_acc: 0.7280\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.3334 - acc: 0.7115 - val_loss: 1.3225 - val_acc: 0.7300\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.3287 - acc: 0.7116 - val_loss: 1.3175 - val_acc: 0.7400\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3248 - acc: 0.7111 - val_loss: 1.3111 - val_acc: 0.7350\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.3194 - acc: 0.7140 - val_loss: 1.3158 - val_acc: 0.7230\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.3157 - acc: 0.7113 - val_loss: 1.3068 - val_acc: 0.7310\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.3109 - acc: 0.7137 - val_loss: 1.2993 - val_acc: 0.7320\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.3064 - acc: 0.7144 - val_loss: 1.2979 - val_acc: 0.7340\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.3021 - acc: 0.7141 - val_loss: 1.2880 - val_acc: 0.7410\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2971 - acc: 0.7167 - val_loss: 1.2865 - val_acc: 0.7340\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.2929 - acc: 0.7151 - val_loss: 1.2771 - val_acc: 0.7450\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2887 - acc: 0.7151 - val_loss: 1.2795 - val_acc: 0.7380\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2850 - acc: 0.7163 - val_loss: 1.2719 - val_acc: 0.7420\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2805 - acc: 0.7147 - val_loss: 1.2753 - val_acc: 0.7340\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2773 - acc: 0.7177 - val_loss: 1.2631 - val_acc: 0.7390\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2725 - acc: 0.7161 - val_loss: 1.2639 - val_acc: 0.7330\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.2682 - acc: 0.7188 - val_loss: 1.2650 - val_acc: 0.7390\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2648 - acc: 0.7187 - val_loss: 1.2551 - val_acc: 0.7420\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2606 - acc: 0.7195 - val_loss: 1.2473 - val_acc: 0.7460\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.2570 - acc: 0.7184 - val_loss: 1.2445 - val_acc: 0.7420\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2527 - acc: 0.7193 - val_loss: 1.2467 - val_acc: 0.7440\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2494 - acc: 0.7197 - val_loss: 1.2368 - val_acc: 0.7450\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2459 - acc: 0.7208 - val_loss: 1.2373 - val_acc: 0.7360\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.2416 - acc: 0.7201 - val_loss: 1.2273 - val_acc: 0.7460\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.2381 - acc: 0.7197 - val_loss: 1.2278 - val_acc: 0.7450\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2348 - acc: 0.7203 - val_loss: 1.2218 - val_acc: 0.7420\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2315 - acc: 0.7208 - val_loss: 1.2200 - val_acc: 0.7480\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2269 - acc: 0.7207 - val_loss: 1.2131 - val_acc: 0.7510\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2243 - acc: 0.7209 - val_loss: 1.2198 - val_acc: 0.7440\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2207 - acc: 0.7212 - val_loss: 1.2083 - val_acc: 0.7430\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.2169 - acc: 0.7219 - val_loss: 1.2018 - val_acc: 0.7490\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2137 - acc: 0.7216 - val_loss: 1.1997 - val_acc: 0.7520\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.2099 - acc: 0.7225 - val_loss: 1.1985 - val_acc: 0.7490\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2069 - acc: 0.7221 - val_loss: 1.1977 - val_acc: 0.7460\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.2043 - acc: 0.7223 - val_loss: 1.1913 - val_acc: 0.7490\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.2006 - acc: 0.7231 - val_loss: 1.1884 - val_acc: 0.7500\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1976 - acc: 0.7235 - val_loss: 1.1847 - val_acc: 0.7480\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1942 - acc: 0.7236 - val_loss: 1.1886 - val_acc: 0.7490\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1911 - acc: 0.7253 - val_loss: 1.1769 - val_acc: 0.7540\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1881 - acc: 0.7251 - val_loss: 1.1805 - val_acc: 0.7490\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1851 - acc: 0.7256 - val_loss: 1.1715 - val_acc: 0.7520\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1814 - acc: 0.7256 - val_loss: 1.1754 - val_acc: 0.7470\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1794 - acc: 0.7255 - val_loss: 1.1736 - val_acc: 0.7460\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1760 - acc: 0.7259 - val_loss: 1.1636 - val_acc: 0.7550\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1733 - acc: 0.7271 - val_loss: 1.1613 - val_acc: 0.7550\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1711 - acc: 0.7269 - val_loss: 1.1583 - val_acc: 0.7520\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1684 - acc: 0.7267 - val_loss: 1.1583 - val_acc: 0.7510\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1648 - acc: 0.7268 - val_loss: 1.1540 - val_acc: 0.7540\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1629 - acc: 0.7289 - val_loss: 1.1518 - val_acc: 0.7520\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1597 - acc: 0.7280 - val_loss: 1.1468 - val_acc: 0.7530\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1571 - acc: 0.7265 - val_loss: 1.1476 - val_acc: 0.7520\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1546 - acc: 0.7288 - val_loss: 1.1445 - val_acc: 0.7500\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.1528 - acc: 0.7276 - val_loss: 1.1405 - val_acc: 0.7550\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1499 - acc: 0.7292 - val_loss: 1.1353 - val_acc: 0.7540\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1479 - acc: 0.7312 - val_loss: 1.1389 - val_acc: 0.7530\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1456 - acc: 0.7308 - val_loss: 1.1303 - val_acc: 0.7570\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1434 - acc: 0.7291 - val_loss: 1.1318 - val_acc: 0.7550\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.1413 - acc: 0.7311 - val_loss: 1.1279 - val_acc: 0.7570\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1392 - acc: 0.7307 - val_loss: 1.1270 - val_acc: 0.7550\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1370 - acc: 0.7300 - val_loss: 1.1244 - val_acc: 0.7550\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1347 - acc: 0.7309 - val_loss: 1.1237 - val_acc: 0.7510\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1329 - acc: 0.7307 - val_loss: 1.1188 - val_acc: 0.7550\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1304 - acc: 0.7317 - val_loss: 1.1201 - val_acc: 0.7600\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1291 - acc: 0.7300 - val_loss: 1.1144 - val_acc: 0.7580\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1269 - acc: 0.7299 - val_loss: 1.1124 - val_acc: 0.7560\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.1252 - acc: 0.7316 - val_loss: 1.1128 - val_acc: 0.7630\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1233 - acc: 0.7328 - val_loss: 1.1089 - val_acc: 0.7580\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1214 - acc: 0.7316 - val_loss: 1.1095 - val_acc: 0.7570\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.1199 - acc: 0.7325 - val_loss: 1.1070 - val_acc: 0.7590\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1181 - acc: 0.7321 - val_loss: 1.1049 - val_acc: 0.7520\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1161 - acc: 0.7335 - val_loss: 1.1014 - val_acc: 0.7570\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1150 - acc: 0.7328 - val_loss: 1.1001 - val_acc: 0.7560\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.1137 - acc: 0.7344 - val_loss: 1.1110 - val_acc: 0.7540\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.1122 - acc: 0.7341 - val_loss: 1.0974 - val_acc: 0.7590\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.1107 - acc: 0.7340 - val_loss: 1.0965 - val_acc: 0.7580\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1090 - acc: 0.7352 - val_loss: 1.0981 - val_acc: 0.7610\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.1073 - acc: 0.7345 - val_loss: 1.0940 - val_acc: 0.7560\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1061 - acc: 0.7357 - val_loss: 1.0925 - val_acc: 0.7650\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1044 - acc: 0.7337 - val_loss: 1.1074 - val_acc: 0.7540\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.1041 - acc: 0.7380 - val_loss: 1.0899 - val_acc: 0.7590\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.1018 - acc: 0.7335 - val_loss: 1.0884 - val_acc: 0.7570\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1004 - acc: 0.7360 - val_loss: 1.0856 - val_acc: 0.7620\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0989 - acc: 0.7352 - val_loss: 1.0855 - val_acc: 0.7620\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0974 - acc: 0.7369 - val_loss: 1.0903 - val_acc: 0.7610\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0965 - acc: 0.7369 - val_loss: 1.0816 - val_acc: 0.7620\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0947 - acc: 0.7361 - val_loss: 1.0857 - val_acc: 0.7580\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0933 - acc: 0.7357 - val_loss: 1.0847 - val_acc: 0.7600\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.0919 - acc: 0.7384 - val_loss: 1.0779 - val_acc: 0.7650\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.0912 - acc: 0.7373 - val_loss: 1.0821 - val_acc: 0.7600\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0900 - acc: 0.7355 - val_loss: 1.0753 - val_acc: 0.7650\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0895 - acc: 0.7368 - val_loss: 1.0740 - val_acc: 0.7630\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0872 - acc: 0.7373 - val_loss: 1.0727 - val_acc: 0.7610\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0864 - acc: 0.7388 - val_loss: 1.0718 - val_acc: 0.7630\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0852 - acc: 0.7365 - val_loss: 1.0734 - val_acc: 0.7600\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.0846 - acc: 0.7395 - val_loss: 1.0726 - val_acc: 0.7640\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0833 - acc: 0.7388 - val_loss: 1.0737 - val_acc: 0.7680\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0827 - acc: 0.7392 - val_loss: 1.0673 - val_acc: 0.7680\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0816 - acc: 0.7381 - val_loss: 1.0730 - val_acc: 0.7650\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0796 - acc: 0.7388 - val_loss: 1.0686 - val_acc: 0.7670\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0785 - acc: 0.7399 - val_loss: 1.0811 - val_acc: 0.7640\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.0788 - acc: 0.7393 - val_loss: 1.0654 - val_acc: 0.7670\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 1s 183us/step - loss: 1.0765 - acc: 0.7397 - val_loss: 1.0707 - val_acc: 0.7670\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.0755 - acc: 0.7404 - val_loss: 1.0618 - val_acc: 0.7630\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0744 - acc: 0.7411 - val_loss: 1.0592 - val_acc: 0.7680\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0738 - acc: 0.7384 - val_loss: 1.0620 - val_acc: 0.7640\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0728 - acc: 0.7384 - val_loss: 1.0609 - val_acc: 0.7650\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0719 - acc: 0.7397 - val_loss: 1.0577 - val_acc: 0.7620\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0704 - acc: 0.7412 - val_loss: 1.0580 - val_acc: 0.7620\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.0692 - acc: 0.7419 - val_loss: 1.0610 - val_acc: 0.7590\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0690 - acc: 0.7413 - val_loss: 1.0593 - val_acc: 0.7680\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0677 - acc: 0.7403 - val_loss: 1.0622 - val_acc: 0.7700\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0670 - acc: 0.7412 - val_loss: 1.0521 - val_acc: 0.7670\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0658 - acc: 0.7409 - val_loss: 1.0527 - val_acc: 0.7620\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.0654 - acc: 0.7413 - val_loss: 1.0521 - val_acc: 0.7650\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.0640 - acc: 0.7420 - val_loss: 1.0534 - val_acc: 0.7640\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0639 - acc: 0.7413 - val_loss: 1.0498 - val_acc: 0.7670\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0618 - acc: 0.7423 - val_loss: 1.0525 - val_acc: 0.7700\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0620 - acc: 0.7423 - val_loss: 1.0487 - val_acc: 0.7680\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0605 - acc: 0.7425 - val_loss: 1.0507 - val_acc: 0.7690\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0598 - acc: 0.7428 - val_loss: 1.0464 - val_acc: 0.7710\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0588 - acc: 0.7436 - val_loss: 1.0440 - val_acc: 0.7650\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0577 - acc: 0.7431 - val_loss: 1.0511 - val_acc: 0.7600\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0570 - acc: 0.7423 - val_loss: 1.0518 - val_acc: 0.7690\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0562 - acc: 0.7429 - val_loss: 1.0443 - val_acc: 0.7720\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0550 - acc: 0.7415 - val_loss: 1.0413 - val_acc: 0.7720\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0544 - acc: 0.7436 - val_loss: 1.0503 - val_acc: 0.7700\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0536 - acc: 0.7436 - val_loss: 1.0479 - val_acc: 0.7660\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0529 - acc: 0.7447 - val_loss: 1.0448 - val_acc: 0.7690\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0520 - acc: 0.7449 - val_loss: 1.0392 - val_acc: 0.7740\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0511 - acc: 0.7461 - val_loss: 1.0380 - val_acc: 0.7700\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0504 - acc: 0.7439 - val_loss: 1.0428 - val_acc: 0.7650\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0490 - acc: 0.7433 - val_loss: 1.0353 - val_acc: 0.7690\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0489 - acc: 0.7433 - val_loss: 1.0353 - val_acc: 0.7690\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0478 - acc: 0.7435 - val_loss: 1.0369 - val_acc: 0.7720\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0475 - acc: 0.7445 - val_loss: 1.0378 - val_acc: 0.7680\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0465 - acc: 0.7441 - val_loss: 1.0337 - val_acc: 0.7710\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0453 - acc: 0.7444 - val_loss: 1.0385 - val_acc: 0.7670\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 1.0449 - acc: 0.7461 - val_loss: 1.0328 - val_acc: 0.7690\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0441 - acc: 0.7468 - val_loss: 1.0326 - val_acc: 0.7660\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0433 - acc: 0.7448 - val_loss: 1.0353 - val_acc: 0.7740\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0432 - acc: 0.7456 - val_loss: 1.0304 - val_acc: 0.7660\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0422 - acc: 0.7459 - val_loss: 1.0334 - val_acc: 0.7670\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0411 - acc: 0.7463 - val_loss: 1.0300 - val_acc: 0.7700\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0400 - acc: 0.7463 - val_loss: 1.0285 - val_acc: 0.7690\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0395 - acc: 0.7447 - val_loss: 1.0281 - val_acc: 0.7680\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0384 - acc: 0.7480 - val_loss: 1.0262 - val_acc: 0.7730\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.0380 - acc: 0.7465 - val_loss: 1.0330 - val_acc: 0.7700\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.0368 - acc: 0.7468 - val_loss: 1.0283 - val_acc: 0.7710\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 1.0372 - acc: 0.7463 - val_loss: 1.0413 - val_acc: 0.7620\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0357 - acc: 0.7465 - val_loss: 1.0233 - val_acc: 0.7720\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0345 - acc: 0.7473 - val_loss: 1.0239 - val_acc: 0.7710\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0341 - acc: 0.7480 - val_loss: 1.0312 - val_acc: 0.7660\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0342 - acc: 0.7461 - val_loss: 1.0253 - val_acc: 0.7680\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0330 - acc: 0.7491 - val_loss: 1.0253 - val_acc: 0.7660\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0317 - acc: 0.7485 - val_loss: 1.0228 - val_acc: 0.7760\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0313 - acc: 0.7484 - val_loss: 1.0211 - val_acc: 0.7660\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0308 - acc: 0.7491 - val_loss: 1.0223 - val_acc: 0.7680\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0300 - acc: 0.7479 - val_loss: 1.0180 - val_acc: 0.7720\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 1.0286 - acc: 0.7476 - val_loss: 1.0264 - val_acc: 0.7730\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0284 - acc: 0.7469 - val_loss: 1.0189 - val_acc: 0.7670\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0278 - acc: 0.7484 - val_loss: 1.0236 - val_acc: 0.7630\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0269 - acc: 0.7483 - val_loss: 1.0159 - val_acc: 0.7740\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0255 - acc: 0.7488 - val_loss: 1.0142 - val_acc: 0.7750\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0251 - acc: 0.7476 - val_loss: 1.0185 - val_acc: 0.7660\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0248 - acc: 0.7501 - val_loss: 1.0142 - val_acc: 0.7760\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0240 - acc: 0.7493 - val_loss: 1.0161 - val_acc: 0.7690\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.0230 - acc: 0.7461 - val_loss: 1.0153 - val_acc: 0.7690\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0225 - acc: 0.7493 - val_loss: 1.0179 - val_acc: 0.7720\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0224 - acc: 0.7484 - val_loss: 1.0176 - val_acc: 0.7670\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0214 - acc: 0.7487 - val_loss: 1.0110 - val_acc: 0.7710\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0203 - acc: 0.7476 - val_loss: 1.0129 - val_acc: 0.7710\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0205 - acc: 0.7495 - val_loss: 1.0127 - val_acc: 0.7710\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0196 - acc: 0.7488 - val_loss: 1.0137 - val_acc: 0.7700\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0188 - acc: 0.7505 - val_loss: 1.0080 - val_acc: 0.7720\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0180 - acc: 0.7491 - val_loss: 1.0065 - val_acc: 0.7720\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0174 - acc: 0.7497 - val_loss: 1.0163 - val_acc: 0.7720\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0171 - acc: 0.7489 - val_loss: 1.0121 - val_acc: 0.7650\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0160 - acc: 0.7481 - val_loss: 1.0070 - val_acc: 0.7730\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0155 - acc: 0.7485 - val_loss: 1.0220 - val_acc: 0.7640\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.0149 - acc: 0.7504 - val_loss: 1.0055 - val_acc: 0.7730\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0141 - acc: 0.7511 - val_loss: 1.0088 - val_acc: 0.7680\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0139 - acc: 0.7504 - val_loss: 1.0051 - val_acc: 0.7720\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.0133 - acc: 0.7493 - val_loss: 1.0060 - val_acc: 0.7740\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 1.0116 - acc: 0.7484 - val_loss: 1.0027 - val_acc: 0.7740\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0117 - acc: 0.7504 - val_loss: 1.0077 - val_acc: 0.7670\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 1.0132 - acc: 0.7512 - val_loss: 1.0029 - val_acc: 0.7720\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0109 - acc: 0.7503 - val_loss: 1.0061 - val_acc: 0.7690\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0100 - acc: 0.7508 - val_loss: 1.0037 - val_acc: 0.7710\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.0098 - acc: 0.7505 - val_loss: 1.0005 - val_acc: 0.7670\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0083 - acc: 0.7515 - val_loss: 1.0016 - val_acc: 0.7710\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0083 - acc: 0.7504 - val_loss: 1.0038 - val_acc: 0.7700\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0078 - acc: 0.7517 - val_loss: 0.9990 - val_acc: 0.7700\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.0067 - acc: 0.7539 - val_loss: 1.0014 - val_acc: 0.7680\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0066 - acc: 0.7517 - val_loss: 0.9991 - val_acc: 0.7740\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0056 - acc: 0.7517 - val_loss: 0.9981 - val_acc: 0.7750\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.0056 - acc: 0.7513 - val_loss: 1.0132 - val_acc: 0.7680\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.0056 - acc: 0.7544 - val_loss: 0.9976 - val_acc: 0.7710\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0047 - acc: 0.7523 - val_loss: 0.9958 - val_acc: 0.7700\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 1.0035 - acc: 0.7539 - val_loss: 0.9972 - val_acc: 0.7690\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 1.0030 - acc: 0.7533 - val_loss: 0.9942 - val_acc: 0.7760\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.0021 - acc: 0.7517 - val_loss: 0.9955 - val_acc: 0.7770\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0011 - acc: 0.7531 - val_loss: 1.0007 - val_acc: 0.7750\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 1.0014 - acc: 0.7516 - val_loss: 0.9948 - val_acc: 0.7740\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 1.0008 - acc: 0.7539 - val_loss: 0.9943 - val_acc: 0.7790\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.0000 - acc: 0.7527 - val_loss: 0.9950 - val_acc: 0.7740\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9992 - acc: 0.7540 - val_loss: 1.0013 - val_acc: 0.7720\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9991 - acc: 0.7537 - val_loss: 0.9948 - val_acc: 0.7750\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9988 - acc: 0.7521 - val_loss: 0.9995 - val_acc: 0.7730\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9990 - acc: 0.7532 - val_loss: 0.9909 - val_acc: 0.7720\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.9972 - acc: 0.7548 - val_loss: 0.9940 - val_acc: 0.7720\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9969 - acc: 0.7543 - val_loss: 0.9962 - val_acc: 0.7730\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9967 - acc: 0.7540 - val_loss: 0.9897 - val_acc: 0.7680\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9962 - acc: 0.7541 - val_loss: 0.9936 - val_acc: 0.7710\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9956 - acc: 0.7553 - val_loss: 1.0032 - val_acc: 0.7670\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9951 - acc: 0.7529 - val_loss: 0.9894 - val_acc: 0.7690\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9946 - acc: 0.7551 - val_loss: 0.9912 - val_acc: 0.7770\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9941 - acc: 0.7540 - val_loss: 0.9873 - val_acc: 0.7720\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9937 - acc: 0.7524 - val_loss: 0.9892 - val_acc: 0.7750\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9933 - acc: 0.7561 - val_loss: 0.9956 - val_acc: 0.7680\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9928 - acc: 0.7547 - val_loss: 0.9869 - val_acc: 0.7760\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9923 - acc: 0.7548 - val_loss: 0.9891 - val_acc: 0.7720\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9916 - acc: 0.7529 - val_loss: 0.9878 - val_acc: 0.7740\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9909 - acc: 0.7552 - val_loss: 0.9880 - val_acc: 0.7770\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9904 - acc: 0.7549 - val_loss: 0.9875 - val_acc: 0.7730\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9915 - acc: 0.7540 - val_loss: 0.9879 - val_acc: 0.7780\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9896 - acc: 0.7557 - val_loss: 0.9834 - val_acc: 0.7710\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9885 - acc: 0.7559 - val_loss: 0.9888 - val_acc: 0.7700\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9885 - acc: 0.7539 - val_loss: 0.9861 - val_acc: 0.7810\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9888 - acc: 0.7548 - val_loss: 0.9824 - val_acc: 0.7730\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9883 - acc: 0.7551 - val_loss: 0.9842 - val_acc: 0.7740\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9873 - acc: 0.7557 - val_loss: 0.9850 - val_acc: 0.7710\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9870 - acc: 0.7560 - val_loss: 0.9801 - val_acc: 0.7730\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9861 - acc: 0.7549 - val_loss: 0.9879 - val_acc: 0.7820\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9854 - acc: 0.7565 - val_loss: 0.9871 - val_acc: 0.7720\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9862 - acc: 0.7565 - val_loss: 0.9940 - val_acc: 0.7720\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9853 - acc: 0.7580 - val_loss: 0.9868 - val_acc: 0.7750\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9840 - acc: 0.7551 - val_loss: 0.9806 - val_acc: 0.7700\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9849 - acc: 0.7545 - val_loss: 0.9848 - val_acc: 0.7690\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9856 - acc: 0.7564 - val_loss: 0.9787 - val_acc: 0.7770\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9835 - acc: 0.7579 - val_loss: 0.9771 - val_acc: 0.7820\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9821 - acc: 0.7567 - val_loss: 0.9804 - val_acc: 0.7770\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9826 - acc: 0.7568 - val_loss: 0.9769 - val_acc: 0.7780\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9817 - acc: 0.7573 - val_loss: 0.9763 - val_acc: 0.7710\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9817 - acc: 0.7584 - val_loss: 0.9793 - val_acc: 0.7750\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9816 - acc: 0.7572 - val_loss: 0.9777 - val_acc: 0.7700\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9811 - acc: 0.7585 - val_loss: 0.9874 - val_acc: 0.7670\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9810 - acc: 0.7573 - val_loss: 0.9846 - val_acc: 0.7750\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9792 - acc: 0.7583 - val_loss: 0.9741 - val_acc: 0.7780\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9792 - acc: 0.7571 - val_loss: 0.9785 - val_acc: 0.7780\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9784 - acc: 0.7557 - val_loss: 0.9745 - val_acc: 0.7770\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9784 - acc: 0.7568 - val_loss: 0.9744 - val_acc: 0.7700\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9782 - acc: 0.7564 - val_loss: 0.9804 - val_acc: 0.7720\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9781 - acc: 0.7569 - val_loss: 0.9722 - val_acc: 0.7750\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9773 - acc: 0.7585 - val_loss: 0.9751 - val_acc: 0.7700\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9782 - acc: 0.7571 - val_loss: 0.9809 - val_acc: 0.7680\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9774 - acc: 0.7561 - val_loss: 0.9791 - val_acc: 0.7750\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9764 - acc: 0.7573 - val_loss: 0.9735 - val_acc: 0.7810\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9758 - acc: 0.7568 - val_loss: 0.9746 - val_acc: 0.7750\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9758 - acc: 0.7591 - val_loss: 0.9838 - val_acc: 0.7750\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9749 - acc: 0.7572 - val_loss: 0.9761 - val_acc: 0.7760\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9749 - acc: 0.7589 - val_loss: 0.9710 - val_acc: 0.7750\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9739 - acc: 0.7577 - val_loss: 0.9697 - val_acc: 0.7790\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9731 - acc: 0.7591 - val_loss: 0.9695 - val_acc: 0.7750\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9737 - acc: 0.7577 - val_loss: 0.9710 - val_acc: 0.7700\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9728 - acc: 0.7592 - val_loss: 0.9749 - val_acc: 0.7770\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9715 - acc: 0.7596 - val_loss: 0.9690 - val_acc: 0.7790\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9722 - acc: 0.7591 - val_loss: 0.9690 - val_acc: 0.7780\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.9721 - acc: 0.7577 - val_loss: 0.9768 - val_acc: 0.7790\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9718 - acc: 0.7600 - val_loss: 0.9700 - val_acc: 0.7660\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9714 - acc: 0.7588 - val_loss: 0.9668 - val_acc: 0.7750\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9716 - acc: 0.7596 - val_loss: 0.9682 - val_acc: 0.7770\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9700 - acc: 0.7593 - val_loss: 0.9723 - val_acc: 0.7740\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9704 - acc: 0.7576 - val_loss: 0.9735 - val_acc: 0.7760\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9699 - acc: 0.7599 - val_loss: 0.9676 - val_acc: 0.7780\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.9697 - acc: 0.7589 - val_loss: 0.9729 - val_acc: 0.7720\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9693 - acc: 0.7585 - val_loss: 0.9663 - val_acc: 0.7750\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9675 - acc: 0.7605 - val_loss: 0.9689 - val_acc: 0.7760\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9679 - acc: 0.7605 - val_loss: 0.9645 - val_acc: 0.7790\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9680 - acc: 0.7587 - val_loss: 0.9689 - val_acc: 0.7750\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9669 - acc: 0.7603 - val_loss: 0.9651 - val_acc: 0.7820\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9666 - acc: 0.7595 - val_loss: 0.9666 - val_acc: 0.7790\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9663 - acc: 0.7604 - val_loss: 0.9702 - val_acc: 0.7760\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9664 - acc: 0.7597 - val_loss: 0.9741 - val_acc: 0.7780\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9664 - acc: 0.7612 - val_loss: 0.9669 - val_acc: 0.7740\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9652 - acc: 0.7591 - val_loss: 0.9687 - val_acc: 0.7780\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9656 - acc: 0.7593 - val_loss: 0.9804 - val_acc: 0.7640\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9650 - acc: 0.7605 - val_loss: 0.9715 - val_acc: 0.7750\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9645 - acc: 0.7607 - val_loss: 0.9777 - val_acc: 0.7670\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9648 - acc: 0.7605 - val_loss: 0.9674 - val_acc: 0.7770\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9636 - acc: 0.7607 - val_loss: 0.9635 - val_acc: 0.7780\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9633 - acc: 0.7601 - val_loss: 0.9661 - val_acc: 0.7790\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9634 - acc: 0.7605 - val_loss: 0.9769 - val_acc: 0.7640\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9638 - acc: 0.7612 - val_loss: 0.9668 - val_acc: 0.7800\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9628 - acc: 0.7601 - val_loss: 0.9810 - val_acc: 0.7760\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9626 - acc: 0.7600 - val_loss: 0.9649 - val_acc: 0.7800\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9617 - acc: 0.7612 - val_loss: 0.9699 - val_acc: 0.7800\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9622 - acc: 0.7604 - val_loss: 0.9735 - val_acc: 0.7780\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9620 - acc: 0.7628 - val_loss: 0.9644 - val_acc: 0.7810\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9608 - acc: 0.7619 - val_loss: 0.9629 - val_acc: 0.7770\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.9604 - acc: 0.7608 - val_loss: 0.9704 - val_acc: 0.7760\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9607 - acc: 0.7603 - val_loss: 0.9615 - val_acc: 0.7820\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9600 - acc: 0.7621 - val_loss: 0.9653 - val_acc: 0.7670\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9595 - acc: 0.7628 - val_loss: 0.9634 - val_acc: 0.7740\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9603 - acc: 0.7615 - val_loss: 0.9590 - val_acc: 0.7780\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9582 - acc: 0.7605 - val_loss: 0.9627 - val_acc: 0.7810\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9586 - acc: 0.7609 - val_loss: 0.9576 - val_acc: 0.7760\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9575 - acc: 0.7613 - val_loss: 0.9680 - val_acc: 0.7750\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9585 - acc: 0.7615 - val_loss: 0.9636 - val_acc: 0.7770\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9574 - acc: 0.7612 - val_loss: 0.9582 - val_acc: 0.7760\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9571 - acc: 0.7612 - val_loss: 0.9706 - val_acc: 0.7730\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9572 - acc: 0.7619 - val_loss: 0.9661 - val_acc: 0.7740\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9565 - acc: 0.7609 - val_loss: 0.9584 - val_acc: 0.7760\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9570 - acc: 0.7617 - val_loss: 0.9675 - val_acc: 0.7760\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9561 - acc: 0.7620 - val_loss: 0.9666 - val_acc: 0.7760\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9572 - acc: 0.7611 - val_loss: 0.9751 - val_acc: 0.7740\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9557 - acc: 0.7629 - val_loss: 0.9620 - val_acc: 0.7790\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9548 - acc: 0.7615 - val_loss: 0.9611 - val_acc: 0.7790\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9541 - acc: 0.7615 - val_loss: 0.9568 - val_acc: 0.7750\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9548 - acc: 0.7604 - val_loss: 0.9557 - val_acc: 0.7750\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9547 - acc: 0.7628 - val_loss: 0.9571 - val_acc: 0.7810\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9534 - acc: 0.7631 - val_loss: 0.9579 - val_acc: 0.7700\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9535 - acc: 0.7628 - val_loss: 0.9651 - val_acc: 0.7770\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9533 - acc: 0.7633 - val_loss: 0.9641 - val_acc: 0.7790\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9542 - acc: 0.7619 - val_loss: 0.9606 - val_acc: 0.7750\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9529 - acc: 0.7633 - val_loss: 0.9551 - val_acc: 0.7770\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9526 - acc: 0.7632 - val_loss: 0.9558 - val_acc: 0.7720\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9523 - acc: 0.7645 - val_loss: 0.9617 - val_acc: 0.7780\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9524 - acc: 0.7653 - val_loss: 0.9593 - val_acc: 0.7740\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9520 - acc: 0.7637 - val_loss: 0.9547 - val_acc: 0.7730\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9517 - acc: 0.7625 - val_loss: 0.9633 - val_acc: 0.7810\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9494 - acc: 0.7637 - val_loss: 0.9606 - val_acc: 0.7800\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9516 - acc: 0.7620 - val_loss: 0.9744 - val_acc: 0.7800\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9505 - acc: 0.7633 - val_loss: 0.9541 - val_acc: 0.7810\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9504 - acc: 0.7625 - val_loss: 0.9816 - val_acc: 0.7650\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9505 - acc: 0.7633 - val_loss: 0.9547 - val_acc: 0.7750\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9488 - acc: 0.7631 - val_loss: 0.9555 - val_acc: 0.7740\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9486 - acc: 0.7640 - val_loss: 0.9569 - val_acc: 0.7690\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9487 - acc: 0.7625 - val_loss: 0.9514 - val_acc: 0.7760\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9483 - acc: 0.7649 - val_loss: 0.9513 - val_acc: 0.7710\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9479 - acc: 0.7628 - val_loss: 0.9721 - val_acc: 0.7660\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9483 - acc: 0.7649 - val_loss: 0.9526 - val_acc: 0.7780\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9477 - acc: 0.7641 - val_loss: 0.9546 - val_acc: 0.7780\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9471 - acc: 0.7645 - val_loss: 0.9497 - val_acc: 0.7770\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9457 - acc: 0.7652 - val_loss: 0.9545 - val_acc: 0.7800\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9470 - acc: 0.7628 - val_loss: 0.9524 - val_acc: 0.7720\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9455 - acc: 0.7635 - val_loss: 0.9497 - val_acc: 0.7780\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9458 - acc: 0.7637 - val_loss: 0.9503 - val_acc: 0.7700\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9446 - acc: 0.7647 - val_loss: 0.9503 - val_acc: 0.7770\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9452 - acc: 0.7649 - val_loss: 0.9508 - val_acc: 0.7780\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9445 - acc: 0.7649 - val_loss: 0.9659 - val_acc: 0.7730\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.9449 - acc: 0.7635 - val_loss: 0.9495 - val_acc: 0.7770\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9444 - acc: 0.7640 - val_loss: 0.9537 - val_acc: 0.7770\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9439 - acc: 0.7643 - val_loss: 0.9573 - val_acc: 0.7750\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9441 - acc: 0.7631 - val_loss: 0.9523 - val_acc: 0.7770\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9432 - acc: 0.7657 - val_loss: 0.9512 - val_acc: 0.7750\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9424 - acc: 0.7657 - val_loss: 0.9495 - val_acc: 0.7770\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9438 - acc: 0.7649 - val_loss: 0.9527 - val_acc: 0.7770\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9432 - acc: 0.7659 - val_loss: 0.9493 - val_acc: 0.7750\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9425 - acc: 0.7657 - val_loss: 0.9467 - val_acc: 0.7750\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9413 - acc: 0.7649 - val_loss: 0.9475 - val_acc: 0.7750\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9410 - acc: 0.7647 - val_loss: 0.9463 - val_acc: 0.7770\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9413 - acc: 0.7663 - val_loss: 0.9522 - val_acc: 0.7760\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9416 - acc: 0.7657 - val_loss: 0.9520 - val_acc: 0.7790\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9401 - acc: 0.7647 - val_loss: 0.9571 - val_acc: 0.7670\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9411 - acc: 0.7645 - val_loss: 0.9567 - val_acc: 0.7750\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9399 - acc: 0.7651 - val_loss: 0.9569 - val_acc: 0.7740\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9399 - acc: 0.7655 - val_loss: 0.9458 - val_acc: 0.7730\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9391 - acc: 0.7671 - val_loss: 0.9478 - val_acc: 0.7770\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9393 - acc: 0.7652 - val_loss: 0.9476 - val_acc: 0.7790\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9386 - acc: 0.7648 - val_loss: 0.9505 - val_acc: 0.7770\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9385 - acc: 0.7656 - val_loss: 0.9454 - val_acc: 0.7760\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9386 - acc: 0.7657 - val_loss: 0.9458 - val_acc: 0.7780\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9383 - acc: 0.7657 - val_loss: 0.9470 - val_acc: 0.7770\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9376 - acc: 0.7656 - val_loss: 0.9459 - val_acc: 0.7740\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9376 - acc: 0.7677 - val_loss: 0.9517 - val_acc: 0.7800\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9363 - acc: 0.7652 - val_loss: 0.9446 - val_acc: 0.7720\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9363 - acc: 0.7671 - val_loss: 0.9476 - val_acc: 0.7740\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9361 - acc: 0.7680 - val_loss: 0.9515 - val_acc: 0.7790\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9365 - acc: 0.7661 - val_loss: 0.9653 - val_acc: 0.7640\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9374 - acc: 0.7659 - val_loss: 0.9571 - val_acc: 0.7630\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9370 - acc: 0.7659 - val_loss: 0.9539 - val_acc: 0.7770\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9363 - acc: 0.7661 - val_loss: 0.9420 - val_acc: 0.7800\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9359 - acc: 0.7660 - val_loss: 0.9448 - val_acc: 0.7780\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9352 - acc: 0.7664 - val_loss: 0.9494 - val_acc: 0.7790\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9347 - acc: 0.7665 - val_loss: 0.9432 - val_acc: 0.7820\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9343 - acc: 0.7673 - val_loss: 0.9459 - val_acc: 0.7760\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9343 - acc: 0.7683 - val_loss: 0.9460 - val_acc: 0.7780\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9346 - acc: 0.7672 - val_loss: 0.9418 - val_acc: 0.7750\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9330 - acc: 0.7661 - val_loss: 0.9467 - val_acc: 0.7760\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9330 - acc: 0.7652 - val_loss: 0.9401 - val_acc: 0.7760\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9327 - acc: 0.7685 - val_loss: 0.9507 - val_acc: 0.7690\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9335 - acc: 0.7671 - val_loss: 0.9390 - val_acc: 0.7770\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.9325 - acc: 0.7687 - val_loss: 0.9408 - val_acc: 0.7800\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9327 - acc: 0.7671 - val_loss: 0.9469 - val_acc: 0.7840\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9323 - acc: 0.7688 - val_loss: 0.9548 - val_acc: 0.7700\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9330 - acc: 0.7681 - val_loss: 0.9471 - val_acc: 0.7790\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9309 - acc: 0.7704 - val_loss: 0.9403 - val_acc: 0.7740\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9308 - acc: 0.7700 - val_loss: 0.9480 - val_acc: 0.7790\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9322 - acc: 0.7681 - val_loss: 0.9416 - val_acc: 0.7740\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9304 - acc: 0.7677 - val_loss: 0.9393 - val_acc: 0.7760\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9302 - acc: 0.7673 - val_loss: 0.9435 - val_acc: 0.7800\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9305 - acc: 0.7677 - val_loss: 0.9495 - val_acc: 0.7790\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9312 - acc: 0.7675 - val_loss: 0.9523 - val_acc: 0.7770\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9298 - acc: 0.7680 - val_loss: 0.9363 - val_acc: 0.7760\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9298 - acc: 0.7667 - val_loss: 0.9548 - val_acc: 0.7740\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9290 - acc: 0.7683 - val_loss: 0.9554 - val_acc: 0.7730\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9300 - acc: 0.7679 - val_loss: 0.9390 - val_acc: 0.7740\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9297 - acc: 0.7693 - val_loss: 0.9365 - val_acc: 0.7780\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9277 - acc: 0.7689 - val_loss: 0.9662 - val_acc: 0.7540\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9297 - acc: 0.7687 - val_loss: 0.9376 - val_acc: 0.7720\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9288 - acc: 0.7688 - val_loss: 0.9438 - val_acc: 0.7740\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9277 - acc: 0.7695 - val_loss: 0.9438 - val_acc: 0.7650\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9278 - acc: 0.7691 - val_loss: 0.9428 - val_acc: 0.7700\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9293 - acc: 0.7687 - val_loss: 0.9408 - val_acc: 0.7760\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9273 - acc: 0.7685 - val_loss: 0.9376 - val_acc: 0.7730\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9276 - acc: 0.7671 - val_loss: 0.9476 - val_acc: 0.7760\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9278 - acc: 0.7680 - val_loss: 0.9431 - val_acc: 0.7740\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9268 - acc: 0.7693 - val_loss: 0.9411 - val_acc: 0.7760\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9257 - acc: 0.7709 - val_loss: 0.9359 - val_acc: 0.7740\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9260 - acc: 0.7697 - val_loss: 0.9457 - val_acc: 0.7800\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9252 - acc: 0.7704 - val_loss: 0.9414 - val_acc: 0.7720\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9254 - acc: 0.7684 - val_loss: 0.9397 - val_acc: 0.7670\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9249 - acc: 0.7709 - val_loss: 0.9392 - val_acc: 0.7740\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9251 - acc: 0.7688 - val_loss: 0.9451 - val_acc: 0.7760\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9249 - acc: 0.7707 - val_loss: 0.9380 - val_acc: 0.7800\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9241 - acc: 0.7691 - val_loss: 0.9398 - val_acc: 0.7820\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9239 - acc: 0.7703 - val_loss: 0.9419 - val_acc: 0.7800\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9242 - acc: 0.7677 - val_loss: 0.9374 - val_acc: 0.7770\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9244 - acc: 0.7708 - val_loss: 0.9362 - val_acc: 0.7740\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9237 - acc: 0.7717 - val_loss: 0.9353 - val_acc: 0.7770\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9229 - acc: 0.7711 - val_loss: 0.9347 - val_acc: 0.7820\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9217 - acc: 0.7700 - val_loss: 0.9406 - val_acc: 0.7670\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9241 - acc: 0.7705 - val_loss: 0.9368 - val_acc: 0.7780\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9225 - acc: 0.7701 - val_loss: 0.9376 - val_acc: 0.7780\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9232 - acc: 0.7695 - val_loss: 0.9385 - val_acc: 0.7800\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9211 - acc: 0.7692 - val_loss: 0.9325 - val_acc: 0.7750\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9218 - acc: 0.7703 - val_loss: 0.9395 - val_acc: 0.7680\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9217 - acc: 0.7731 - val_loss: 0.9399 - val_acc: 0.7770\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.9218 - acc: 0.7728 - val_loss: 0.9343 - val_acc: 0.7770\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9204 - acc: 0.7723 - val_loss: 0.9433 - val_acc: 0.7770\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9217 - acc: 0.7695 - val_loss: 0.9436 - val_acc: 0.7770\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9214 - acc: 0.7708 - val_loss: 0.9315 - val_acc: 0.7760\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9199 - acc: 0.7715 - val_loss: 0.9355 - val_acc: 0.7790\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9201 - acc: 0.7735 - val_loss: 0.9426 - val_acc: 0.7740\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9198 - acc: 0.7719 - val_loss: 0.9305 - val_acc: 0.7770\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9192 - acc: 0.7715 - val_loss: 0.9338 - val_acc: 0.7780\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9182 - acc: 0.7720 - val_loss: 0.9420 - val_acc: 0.7750\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9194 - acc: 0.7700 - val_loss: 0.9341 - val_acc: 0.7710\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9190 - acc: 0.7712 - val_loss: 0.9357 - val_acc: 0.7700\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9186 - acc: 0.7720 - val_loss: 0.9298 - val_acc: 0.7760\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9178 - acc: 0.7705 - val_loss: 0.9318 - val_acc: 0.7730\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9189 - acc: 0.7729 - val_loss: 0.9355 - val_acc: 0.7720\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9181 - acc: 0.7715 - val_loss: 0.9337 - val_acc: 0.7780\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9182 - acc: 0.7708 - val_loss: 0.9332 - val_acc: 0.7790\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9174 - acc: 0.7721 - val_loss: 0.9315 - val_acc: 0.7780\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9168 - acc: 0.7725 - val_loss: 0.9339 - val_acc: 0.7780\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9160 - acc: 0.7717 - val_loss: 0.9328 - val_acc: 0.7800\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.9174 - acc: 0.7728 - val_loss: 0.9374 - val_acc: 0.7830\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9160 - acc: 0.7713 - val_loss: 0.9541 - val_acc: 0.7670\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9161 - acc: 0.7741 - val_loss: 0.9354 - val_acc: 0.7790\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9160 - acc: 0.7723 - val_loss: 0.9345 - val_acc: 0.7750\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9153 - acc: 0.7739 - val_loss: 0.9285 - val_acc: 0.7800\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9151 - acc: 0.7709 - val_loss: 0.9299 - val_acc: 0.7800\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9155 - acc: 0.7725 - val_loss: 0.9314 - val_acc: 0.7790\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.9142 - acc: 0.7721 - val_loss: 0.9371 - val_acc: 0.7800\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9142 - acc: 0.7731 - val_loss: 0.9288 - val_acc: 0.7770\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.9139 - acc: 0.7729 - val_loss: 0.9267 - val_acc: 0.7780\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9140 - acc: 0.7717 - val_loss: 0.9301 - val_acc: 0.7740\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9132 - acc: 0.7731 - val_loss: 0.9315 - val_acc: 0.7720\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9125 - acc: 0.7732 - val_loss: 0.9287 - val_acc: 0.7820\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9136 - acc: 0.7735 - val_loss: 0.9348 - val_acc: 0.7780\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.9136 - acc: 0.7727 - val_loss: 0.9295 - val_acc: 0.7820\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9130 - acc: 0.7739 - val_loss: 0.9303 - val_acc: 0.7790\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9119 - acc: 0.7733 - val_loss: 0.9408 - val_acc: 0.7780\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9123 - acc: 0.7743 - val_loss: 0.9323 - val_acc: 0.7800\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9124 - acc: 0.7715 - val_loss: 0.9258 - val_acc: 0.7780\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9120 - acc: 0.7743 - val_loss: 0.9308 - val_acc: 0.7830\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.9114 - acc: 0.7740 - val_loss: 0.9322 - val_acc: 0.7740\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.9116 - acc: 0.7721 - val_loss: 0.9333 - val_acc: 0.7780\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9104 - acc: 0.7735 - val_loss: 0.9395 - val_acc: 0.7780\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9111 - acc: 0.7760 - val_loss: 0.9303 - val_acc: 0.7790\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9098 - acc: 0.7752 - val_loss: 0.9300 - val_acc: 0.7800\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9103 - acc: 0.7733 - val_loss: 0.9307 - val_acc: 0.7750\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.9097 - acc: 0.7739 - val_loss: 0.9469 - val_acc: 0.7680\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9104 - acc: 0.7735 - val_loss: 0.9446 - val_acc: 0.7740\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9104 - acc: 0.7761 - val_loss: 0.9350 - val_acc: 0.7840\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9088 - acc: 0.7748 - val_loss: 0.9265 - val_acc: 0.7790\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9093 - acc: 0.7749 - val_loss: 0.9295 - val_acc: 0.7810\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9089 - acc: 0.7747 - val_loss: 0.9356 - val_acc: 0.7720\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.9087 - acc: 0.7749 - val_loss: 0.9361 - val_acc: 0.7720\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9086 - acc: 0.7747 - val_loss: 0.9318 - val_acc: 0.7750\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9102 - acc: 0.7729 - val_loss: 0.9381 - val_acc: 0.7780\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9094 - acc: 0.7756 - val_loss: 0.9288 - val_acc: 0.7740\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.9088 - acc: 0.7752 - val_loss: 0.9306 - val_acc: 0.7790\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9090 - acc: 0.7743 - val_loss: 0.9250 - val_acc: 0.7790\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.9066 - acc: 0.7752 - val_loss: 0.9267 - val_acc: 0.7800\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.9059 - acc: 0.7776 - val_loss: 0.9436 - val_acc: 0.7660\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9077 - acc: 0.7753 - val_loss: 0.9263 - val_acc: 0.7810\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9071 - acc: 0.7751 - val_loss: 0.9395 - val_acc: 0.7740\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.9071 - acc: 0.7737 - val_loss: 0.9246 - val_acc: 0.7820\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9065 - acc: 0.7751 - val_loss: 0.9386 - val_acc: 0.7680\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9063 - acc: 0.7769 - val_loss: 0.9268 - val_acc: 0.7800\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9050 - acc: 0.7748 - val_loss: 0.9366 - val_acc: 0.7670\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.9068 - acc: 0.7752 - val_loss: 0.9258 - val_acc: 0.7790\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9064 - acc: 0.7749 - val_loss: 0.9226 - val_acc: 0.7750\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.9043 - acc: 0.7753 - val_loss: 0.9240 - val_acc: 0.7740\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.9044 - acc: 0.7761 - val_loss: 0.9335 - val_acc: 0.7700\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9038 - acc: 0.7751 - val_loss: 0.9240 - val_acc: 0.7780\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9037 - acc: 0.7763 - val_loss: 0.9245 - val_acc: 0.7770\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9043 - acc: 0.7736 - val_loss: 0.9289 - val_acc: 0.7750\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9036 - acc: 0.7757 - val_loss: 0.9251 - val_acc: 0.7780\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9026 - acc: 0.7771 - val_loss: 0.9521 - val_acc: 0.7650\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9052 - acc: 0.7743 - val_loss: 0.9355 - val_acc: 0.7790\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9036 - acc: 0.7773 - val_loss: 0.9557 - val_acc: 0.7750\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.9038 - acc: 0.7771 - val_loss: 0.9264 - val_acc: 0.7750\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9021 - acc: 0.7771 - val_loss: 0.9421 - val_acc: 0.7710\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.9025 - acc: 0.7753 - val_loss: 0.9203 - val_acc: 0.7790\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9022 - acc: 0.7793 - val_loss: 0.9312 - val_acc: 0.7690\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9031 - acc: 0.7744 - val_loss: 0.9227 - val_acc: 0.7800\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.9009 - acc: 0.7757 - val_loss: 0.9280 - val_acc: 0.7840\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9028 - acc: 0.7776 - val_loss: 0.9233 - val_acc: 0.7810\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9014 - acc: 0.7787 - val_loss: 0.9223 - val_acc: 0.7790\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.9004 - acc: 0.7781 - val_loss: 0.9214 - val_acc: 0.7750\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9004 - acc: 0.7783 - val_loss: 0.9226 - val_acc: 0.7810\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8995 - acc: 0.7783 - val_loss: 0.9243 - val_acc: 0.7780\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.9012 - acc: 0.7773 - val_loss: 0.9199 - val_acc: 0.7780\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8991 - acc: 0.7760 - val_loss: 0.9214 - val_acc: 0.7740\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.9005 - acc: 0.7779 - val_loss: 0.9184 - val_acc: 0.7780\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8982 - acc: 0.7791 - val_loss: 0.9262 - val_acc: 0.7790\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8993 - acc: 0.7784 - val_loss: 0.9247 - val_acc: 0.7770\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.9007 - acc: 0.7791 - val_loss: 0.9303 - val_acc: 0.7790\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8991 - acc: 0.7780 - val_loss: 0.9264 - val_acc: 0.7820\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8983 - acc: 0.7816 - val_loss: 0.9206 - val_acc: 0.7810\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8977 - acc: 0.7776 - val_loss: 0.9308 - val_acc: 0.7790\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.9009 - acc: 0.7785 - val_loss: 0.9268 - val_acc: 0.7790\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8976 - acc: 0.7799 - val_loss: 0.9206 - val_acc: 0.7830\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8978 - acc: 0.7771 - val_loss: 0.9167 - val_acc: 0.7820\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8974 - acc: 0.7773 - val_loss: 0.9178 - val_acc: 0.7790\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8974 - acc: 0.7800 - val_loss: 0.9206 - val_acc: 0.7800\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8973 - acc: 0.7788 - val_loss: 0.9239 - val_acc: 0.7770\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8973 - acc: 0.7789 - val_loss: 0.9232 - val_acc: 0.7820\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8961 - acc: 0.7776 - val_loss: 0.9327 - val_acc: 0.7800\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8968 - acc: 0.7783 - val_loss: 0.9217 - val_acc: 0.7770\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8973 - acc: 0.7755 - val_loss: 0.9233 - val_acc: 0.7780\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8962 - acc: 0.7793 - val_loss: 0.9303 - val_acc: 0.7770\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8960 - acc: 0.7788 - val_loss: 0.9161 - val_acc: 0.7780\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8955 - acc: 0.7769 - val_loss: 0.9218 - val_acc: 0.7760\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8953 - acc: 0.7796 - val_loss: 0.9218 - val_acc: 0.7800\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8946 - acc: 0.7796 - val_loss: 0.9267 - val_acc: 0.7730\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8943 - acc: 0.7808 - val_loss: 0.9529 - val_acc: 0.7680\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8951 - acc: 0.7803 - val_loss: 0.9234 - val_acc: 0.7710\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8953 - acc: 0.7800 - val_loss: 0.9207 - val_acc: 0.7790\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8936 - acc: 0.7779 - val_loss: 0.9174 - val_acc: 0.7820\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8936 - acc: 0.7801 - val_loss: 0.9200 - val_acc: 0.7830\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8951 - acc: 0.7776 - val_loss: 0.9268 - val_acc: 0.7780\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8939 - acc: 0.7795 - val_loss: 0.9228 - val_acc: 0.7780\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8929 - acc: 0.7772 - val_loss: 0.9315 - val_acc: 0.7840\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8934 - acc: 0.7797 - val_loss: 0.9221 - val_acc: 0.7810\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8925 - acc: 0.7784 - val_loss: 0.9193 - val_acc: 0.7760\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8923 - acc: 0.7813 - val_loss: 0.9247 - val_acc: 0.7700\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8910 - acc: 0.7801 - val_loss: 0.9140 - val_acc: 0.7780\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8918 - acc: 0.7796 - val_loss: 0.9188 - val_acc: 0.7820\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8927 - acc: 0.7779 - val_loss: 0.9267 - val_acc: 0.7720\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8920 - acc: 0.7789 - val_loss: 0.9178 - val_acc: 0.7750\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8912 - acc: 0.7813 - val_loss: 0.9173 - val_acc: 0.7790\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8916 - acc: 0.7813 - val_loss: 0.9146 - val_acc: 0.7750\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8922 - acc: 0.7795 - val_loss: 0.9466 - val_acc: 0.7720\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8942 - acc: 0.7787 - val_loss: 0.9208 - val_acc: 0.7730\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8914 - acc: 0.7789 - val_loss: 0.9200 - val_acc: 0.7780\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8909 - acc: 0.7820 - val_loss: 0.9275 - val_acc: 0.7810\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8907 - acc: 0.7797 - val_loss: 0.9177 - val_acc: 0.7700\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8899 - acc: 0.7784 - val_loss: 0.9118 - val_acc: 0.7740\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8893 - acc: 0.7819 - val_loss: 0.9238 - val_acc: 0.7800\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8899 - acc: 0.7815 - val_loss: 0.9183 - val_acc: 0.7830\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8893 - acc: 0.7811 - val_loss: 0.9214 - val_acc: 0.7750\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8886 - acc: 0.7808 - val_loss: 0.9461 - val_acc: 0.7660\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8907 - acc: 0.7781 - val_loss: 0.9197 - val_acc: 0.7780\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8889 - acc: 0.7815 - val_loss: 0.9143 - val_acc: 0.7860\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8866 - acc: 0.7812 - val_loss: 0.9179 - val_acc: 0.7820\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8888 - acc: 0.7789 - val_loss: 0.9153 - val_acc: 0.7800\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8871 - acc: 0.7828 - val_loss: 0.9468 - val_acc: 0.7580\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8892 - acc: 0.7805 - val_loss: 0.9306 - val_acc: 0.7760\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8880 - acc: 0.7809 - val_loss: 0.9151 - val_acc: 0.7840\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8867 - acc: 0.7808 - val_loss: 0.9113 - val_acc: 0.7800\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8878 - acc: 0.7823 - val_loss: 0.9191 - val_acc: 0.7830\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8866 - acc: 0.7819 - val_loss: 0.9138 - val_acc: 0.7850\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8863 - acc: 0.7827 - val_loss: 0.9426 - val_acc: 0.7710\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8896 - acc: 0.7792 - val_loss: 0.9151 - val_acc: 0.7810\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8873 - acc: 0.7813 - val_loss: 0.9118 - val_acc: 0.7780\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8846 - acc: 0.7837 - val_loss: 0.9186 - val_acc: 0.7710\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8865 - acc: 0.7788 - val_loss: 0.9380 - val_acc: 0.7610\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8855 - acc: 0.7803 - val_loss: 0.9195 - val_acc: 0.7690\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8871 - acc: 0.7808 - val_loss: 0.9151 - val_acc: 0.7740\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8856 - acc: 0.7821 - val_loss: 0.9217 - val_acc: 0.7730\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8852 - acc: 0.7840 - val_loss: 0.9215 - val_acc: 0.7800\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8866 - acc: 0.7813 - val_loss: 0.9185 - val_acc: 0.7810\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8855 - acc: 0.7827 - val_loss: 0.9533 - val_acc: 0.7660\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8857 - acc: 0.7808 - val_loss: 0.9131 - val_acc: 0.7770\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8838 - acc: 0.7825 - val_loss: 0.9122 - val_acc: 0.7800\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8838 - acc: 0.7821 - val_loss: 0.9289 - val_acc: 0.7770\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8831 - acc: 0.7825 - val_loss: 0.9110 - val_acc: 0.7860\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8834 - acc: 0.7817 - val_loss: 0.9115 - val_acc: 0.7830\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8846 - acc: 0.7789 - val_loss: 0.9223 - val_acc: 0.7750\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8823 - acc: 0.7843 - val_loss: 0.9104 - val_acc: 0.7790\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8829 - acc: 0.7809 - val_loss: 0.9262 - val_acc: 0.7730\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8826 - acc: 0.7805 - val_loss: 0.9131 - val_acc: 0.7820\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8833 - acc: 0.7807 - val_loss: 0.9337 - val_acc: 0.7680\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8817 - acc: 0.7844 - val_loss: 0.9247 - val_acc: 0.7790\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8802 - acc: 0.7835 - val_loss: 0.9122 - val_acc: 0.7720\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8832 - acc: 0.7819 - val_loss: 0.9098 - val_acc: 0.7780\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8809 - acc: 0.7855 - val_loss: 0.9108 - val_acc: 0.7800\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8819 - acc: 0.7823 - val_loss: 0.9153 - val_acc: 0.7850\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8811 - acc: 0.7820 - val_loss: 0.9176 - val_acc: 0.7790\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8816 - acc: 0.7799 - val_loss: 0.9099 - val_acc: 0.7820\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8813 - acc: 0.7821 - val_loss: 0.9125 - val_acc: 0.7740\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8804 - acc: 0.7817 - val_loss: 0.9124 - val_acc: 0.7850\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8803 - acc: 0.7809 - val_loss: 0.9145 - val_acc: 0.7800\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8788 - acc: 0.7821 - val_loss: 0.9255 - val_acc: 0.7780\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8803 - acc: 0.7820 - val_loss: 0.9131 - val_acc: 0.7820\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8784 - acc: 0.7812 - val_loss: 0.9223 - val_acc: 0.7790\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8794 - acc: 0.7816 - val_loss: 0.9510 - val_acc: 0.7580\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8806 - acc: 0.7815 - val_loss: 0.9141 - val_acc: 0.7790\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8803 - acc: 0.7829 - val_loss: 0.9275 - val_acc: 0.7700\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8794 - acc: 0.7844 - val_loss: 0.9252 - val_acc: 0.7690\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8773 - acc: 0.7835 - val_loss: 0.9131 - val_acc: 0.7760\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8775 - acc: 0.7828 - val_loss: 0.9073 - val_acc: 0.7850\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8776 - acc: 0.7831 - val_loss: 0.9069 - val_acc: 0.7850\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8759 - acc: 0.7840 - val_loss: 0.9178 - val_acc: 0.7810\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8784 - acc: 0.7845 - val_loss: 0.9090 - val_acc: 0.7750\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8766 - acc: 0.7828 - val_loss: 0.9150 - val_acc: 0.7670\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8789 - acc: 0.7811 - val_loss: 0.9266 - val_acc: 0.7630\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8770 - acc: 0.7839 - val_loss: 0.9134 - val_acc: 0.7880\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8781 - acc: 0.7832 - val_loss: 0.9367 - val_acc: 0.7760\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8784 - acc: 0.7843 - val_loss: 0.9045 - val_acc: 0.7860\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8762 - acc: 0.7851 - val_loss: 0.9079 - val_acc: 0.7850\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8760 - acc: 0.7852 - val_loss: 0.9180 - val_acc: 0.7740\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8763 - acc: 0.7848 - val_loss: 0.9133 - val_acc: 0.7810\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8750 - acc: 0.7855 - val_loss: 0.9124 - val_acc: 0.7770\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8766 - acc: 0.7853 - val_loss: 0.9141 - val_acc: 0.7800\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8739 - acc: 0.7861 - val_loss: 0.9112 - val_acc: 0.7720\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8735 - acc: 0.7861 - val_loss: 0.9305 - val_acc: 0.7660\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8773 - acc: 0.7813 - val_loss: 0.9292 - val_acc: 0.7750\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8747 - acc: 0.7851 - val_loss: 0.9142 - val_acc: 0.7830\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8734 - acc: 0.7852 - val_loss: 0.9360 - val_acc: 0.7710\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8736 - acc: 0.7841 - val_loss: 0.9154 - val_acc: 0.7840\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8737 - acc: 0.7860 - val_loss: 0.9165 - val_acc: 0.7790\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8743 - acc: 0.7839 - val_loss: 0.9388 - val_acc: 0.7620\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8754 - acc: 0.7843 - val_loss: 0.9049 - val_acc: 0.7840\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8713 - acc: 0.7867 - val_loss: 0.9084 - val_acc: 0.7790\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8728 - acc: 0.7833 - val_loss: 0.9114 - val_acc: 0.7700\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8722 - acc: 0.7843 - val_loss: 0.9147 - val_acc: 0.7780\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8726 - acc: 0.7856 - val_loss: 0.9179 - val_acc: 0.7750\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8707 - acc: 0.7867 - val_loss: 0.9016 - val_acc: 0.7790\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8703 - acc: 0.7840 - val_loss: 0.9034 - val_acc: 0.7830\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8732 - acc: 0.7853 - val_loss: 0.9209 - val_acc: 0.7770\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8725 - acc: 0.7816 - val_loss: 0.9008 - val_acc: 0.7800\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8704 - acc: 0.7828 - val_loss: 0.9205 - val_acc: 0.7770\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8714 - acc: 0.7853 - val_loss: 0.9086 - val_acc: 0.7740\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8698 - acc: 0.7876 - val_loss: 0.9046 - val_acc: 0.7730\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8708 - acc: 0.7857 - val_loss: 0.9071 - val_acc: 0.7820\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8710 - acc: 0.7860 - val_loss: 0.9037 - val_acc: 0.7800\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8706 - acc: 0.7856 - val_loss: 0.9013 - val_acc: 0.7760\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8707 - acc: 0.7831 - val_loss: 0.9021 - val_acc: 0.7870\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8696 - acc: 0.7871 - val_loss: 0.9027 - val_acc: 0.7850\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8703 - acc: 0.7819 - val_loss: 0.9527 - val_acc: 0.7600\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8747 - acc: 0.7839 - val_loss: 0.9158 - val_acc: 0.7720\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8708 - acc: 0.7833 - val_loss: 0.9043 - val_acc: 0.7730\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8683 - acc: 0.7852 - val_loss: 0.9001 - val_acc: 0.7840\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8688 - acc: 0.7848 - val_loss: 0.9102 - val_acc: 0.7760\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8676 - acc: 0.7888 - val_loss: 0.9059 - val_acc: 0.7750\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8680 - acc: 0.7844 - val_loss: 0.8994 - val_acc: 0.7800\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8682 - acc: 0.7897 - val_loss: 0.9314 - val_acc: 0.7680\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8710 - acc: 0.7833 - val_loss: 0.8997 - val_acc: 0.7870\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8681 - acc: 0.7857 - val_loss: 0.9015 - val_acc: 0.7800\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8671 - acc: 0.7869 - val_loss: 0.9074 - val_acc: 0.7760\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8673 - acc: 0.7855 - val_loss: 0.9019 - val_acc: 0.7910\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8647 - acc: 0.7871 - val_loss: 0.9019 - val_acc: 0.7740\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8664 - acc: 0.7841 - val_loss: 0.8991 - val_acc: 0.7830\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8643 - acc: 0.7877 - val_loss: 0.9055 - val_acc: 0.7790\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8667 - acc: 0.7849 - val_loss: 0.9020 - val_acc: 0.7860\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8653 - acc: 0.7855 - val_loss: 0.9102 - val_acc: 0.7770\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8653 - acc: 0.7875 - val_loss: 0.8997 - val_acc: 0.7760\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8639 - acc: 0.7892 - val_loss: 0.8997 - val_acc: 0.7830\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8640 - acc: 0.7869 - val_loss: 0.9160 - val_acc: 0.7740\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8669 - acc: 0.7856 - val_loss: 0.9025 - val_acc: 0.7840\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8651 - acc: 0.7888 - val_loss: 0.9062 - val_acc: 0.7690\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8663 - acc: 0.7848 - val_loss: 0.9090 - val_acc: 0.7820\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8636 - acc: 0.7881 - val_loss: 0.9077 - val_acc: 0.7870\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8643 - acc: 0.7883 - val_loss: 0.9272 - val_acc: 0.7710\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8664 - acc: 0.7859 - val_loss: 0.9134 - val_acc: 0.7770\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8643 - acc: 0.7867 - val_loss: 0.9146 - val_acc: 0.7770\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8652 - acc: 0.7871 - val_loss: 0.9160 - val_acc: 0.7710\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8622 - acc: 0.7865 - val_loss: 0.9159 - val_acc: 0.7770\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8631 - acc: 0.7852 - val_loss: 0.9046 - val_acc: 0.7870\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8652 - acc: 0.7883 - val_loss: 0.9042 - val_acc: 0.7710\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8615 - acc: 0.7897 - val_loss: 0.9139 - val_acc: 0.7690\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8626 - acc: 0.7893 - val_loss: 0.9838 - val_acc: 0.7480\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8676 - acc: 0.7835 - val_loss: 0.9077 - val_acc: 0.7790\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8630 - acc: 0.7877 - val_loss: 0.9258 - val_acc: 0.7660\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8631 - acc: 0.7892 - val_loss: 0.8983 - val_acc: 0.7860\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8609 - acc: 0.7883 - val_loss: 0.9073 - val_acc: 0.7800\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8618 - acc: 0.7879 - val_loss: 0.9056 - val_acc: 0.7790\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8614 - acc: 0.7891 - val_loss: 0.9027 - val_acc: 0.7800\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8610 - acc: 0.7899 - val_loss: 0.9116 - val_acc: 0.7800\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8598 - acc: 0.7885 - val_loss: 0.9505 - val_acc: 0.7620\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8634 - acc: 0.7883 - val_loss: 0.9228 - val_acc: 0.7740\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8607 - acc: 0.7881 - val_loss: 0.9019 - val_acc: 0.7850\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 67us/step - loss: 0.8618 - acc: 0.7880 - val_loss: 0.9098 - val_acc: 0.7760\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8593 - acc: 0.7872 - val_loss: 0.8972 - val_acc: 0.7820\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8582 - acc: 0.7876 - val_loss: 0.9176 - val_acc: 0.7750\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8606 - acc: 0.7919 - val_loss: 0.9011 - val_acc: 0.7790\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8599 - acc: 0.7884 - val_loss: 0.9088 - val_acc: 0.7750\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8616 - acc: 0.7871 - val_loss: 0.9239 - val_acc: 0.7730\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8593 - acc: 0.7873 - val_loss: 0.8946 - val_acc: 0.7920\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 0.8595 - acc: 0.7872 - val_loss: 0.9085 - val_acc: 0.7820\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8606 - acc: 0.7881 - val_loss: 0.8955 - val_acc: 0.7820\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8574 - acc: 0.7900 - val_loss: 0.9098 - val_acc: 0.7840\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8618 - acc: 0.7852 - val_loss: 0.8936 - val_acc: 0.7790\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8571 - acc: 0.7920 - val_loss: 0.8959 - val_acc: 0.7810\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8574 - acc: 0.7891 - val_loss: 0.8943 - val_acc: 0.7770\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8607 - acc: 0.7873 - val_loss: 0.8969 - val_acc: 0.7810\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8551 - acc: 0.7904 - val_loss: 0.9123 - val_acc: 0.7790\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8571 - acc: 0.7897 - val_loss: 0.9012 - val_acc: 0.7800\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.8574 - acc: 0.7868 - val_loss: 0.8983 - val_acc: 0.7870\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 0.8585 - acc: 0.7881 - val_loss: 0.8938 - val_acc: 0.7810\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8546 - acc: 0.7909 - val_loss: 0.8945 - val_acc: 0.7860\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8583 - acc: 0.7869 - val_loss: 0.9073 - val_acc: 0.7860\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8552 - acc: 0.7889 - val_loss: 0.8950 - val_acc: 0.7850\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 0.8572 - acc: 0.7923 - val_loss: 0.8983 - val_acc: 0.7860\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8550 - acc: 0.7907 - val_loss: 0.9014 - val_acc: 0.7800\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8572 - acc: 0.7881 - val_loss: 0.9267 - val_acc: 0.7760\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8587 - acc: 0.7844 - val_loss: 0.9105 - val_acc: 0.7750\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8558 - acc: 0.7900 - val_loss: 0.8983 - val_acc: 0.7790\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8560 - acc: 0.7893 - val_loss: 0.8954 - val_acc: 0.7800\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8533 - acc: 0.7908 - val_loss: 0.9114 - val_acc: 0.7680\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8544 - acc: 0.7875 - val_loss: 0.9036 - val_acc: 0.7820\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8528 - acc: 0.7919 - val_loss: 0.8968 - val_acc: 0.7790\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8532 - acc: 0.7908 - val_loss: 0.9053 - val_acc: 0.7760\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8532 - acc: 0.7896 - val_loss: 0.8942 - val_acc: 0.7790\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8515 - acc: 0.7928 - val_loss: 0.9458 - val_acc: 0.7610\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8564 - acc: 0.7877 - val_loss: 0.9136 - val_acc: 0.7850\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8532 - acc: 0.7916 - val_loss: 0.8952 - val_acc: 0.7880\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8533 - acc: 0.7915 - val_loss: 0.9531 - val_acc: 0.7600\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8552 - acc: 0.7903 - val_loss: 0.8995 - val_acc: 0.7840\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8529 - acc: 0.7925 - val_loss: 0.8997 - val_acc: 0.7780\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8514 - acc: 0.7899 - val_loss: 0.8882 - val_acc: 0.7860\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8493 - acc: 0.7907 - val_loss: 0.9018 - val_acc: 0.7850\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8544 - acc: 0.7911 - val_loss: 0.9086 - val_acc: 0.7760\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8528 - acc: 0.7917 - val_loss: 0.9336 - val_acc: 0.7550\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8537 - acc: 0.7888 - val_loss: 0.9409 - val_acc: 0.7590\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8541 - acc: 0.7915 - val_loss: 0.8971 - val_acc: 0.7880\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8525 - acc: 0.7901 - val_loss: 0.9505 - val_acc: 0.7490\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8530 - acc: 0.7903 - val_loss: 0.8930 - val_acc: 0.7810\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8544 - acc: 0.7891 - val_loss: 0.8914 - val_acc: 0.7810\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8502 - acc: 0.7912 - val_loss: 0.9090 - val_acc: 0.7810\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8521 - acc: 0.7915 - val_loss: 0.8920 - val_acc: 0.7930\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8494 - acc: 0.7912 - val_loss: 0.8961 - val_acc: 0.7800\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8497 - acc: 0.7921 - val_loss: 0.8957 - val_acc: 0.7730\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8505 - acc: 0.7933 - val_loss: 0.8949 - val_acc: 0.7830\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8536 - acc: 0.7912 - val_loss: 0.9030 - val_acc: 0.7810\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8500 - acc: 0.7924 - val_loss: 0.9138 - val_acc: 0.7750\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8490 - acc: 0.7913 - val_loss: 0.8888 - val_acc: 0.7790\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8475 - acc: 0.7917 - val_loss: 0.8981 - val_acc: 0.7870\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8483 - acc: 0.7916 - val_loss: 0.8888 - val_acc: 0.7880\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8482 - acc: 0.7939 - val_loss: 0.8936 - val_acc: 0.7840\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8477 - acc: 0.7908 - val_loss: 0.8934 - val_acc: 0.7840\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8563 - acc: 0.7876 - val_loss: 0.9016 - val_acc: 0.7790\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8471 - acc: 0.7957 - val_loss: 0.8937 - val_acc: 0.7900\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8498 - acc: 0.7883 - val_loss: 0.9107 - val_acc: 0.7820\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8478 - acc: 0.7896 - val_loss: 0.9079 - val_acc: 0.7870\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8477 - acc: 0.7937 - val_loss: 0.9072 - val_acc: 0.7730\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8474 - acc: 0.7943 - val_loss: 0.9074 - val_acc: 0.7810\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8471 - acc: 0.7920 - val_loss: 0.9143 - val_acc: 0.7800\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8473 - acc: 0.7903 - val_loss: 0.9564 - val_acc: 0.7570\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8496 - acc: 0.7928 - val_loss: 0.8984 - val_acc: 0.7750\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8478 - acc: 0.7920 - val_loss: 0.9006 - val_acc: 0.7790\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8475 - acc: 0.7909 - val_loss: 0.9021 - val_acc: 0.7880\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8474 - acc: 0.7920 - val_loss: 0.8866 - val_acc: 0.7760\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8475 - acc: 0.7913 - val_loss: 0.8865 - val_acc: 0.7860\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.8476 - acc: 0.7939 - val_loss: 0.8959 - val_acc: 0.7760\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8456 - acc: 0.7936 - val_loss: 0.8846 - val_acc: 0.7890\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8452 - acc: 0.7933 - val_loss: 0.8990 - val_acc: 0.7740\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8438 - acc: 0.7945 - val_loss: 0.9577 - val_acc: 0.7550\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8487 - acc: 0.7913 - val_loss: 0.8884 - val_acc: 0.7870\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8431 - acc: 0.7960 - val_loss: 0.8903 - val_acc: 0.7880\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8478 - acc: 0.7903 - val_loss: 0.9116 - val_acc: 0.7710\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8441 - acc: 0.7943 - val_loss: 0.8946 - val_acc: 0.7800\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8483 - acc: 0.7912 - val_loss: 0.9107 - val_acc: 0.7880\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8438 - acc: 0.7929 - val_loss: 0.9013 - val_acc: 0.7730\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8491 - acc: 0.7912 - val_loss: 0.8995 - val_acc: 0.7710\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8456 - acc: 0.7945 - val_loss: 0.8852 - val_acc: 0.7930\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8413 - acc: 0.7923 - val_loss: 0.8867 - val_acc: 0.7860\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8463 - acc: 0.7940 - val_loss: 0.8976 - val_acc: 0.7830\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8437 - acc: 0.7941 - val_loss: 0.8944 - val_acc: 0.7680\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8450 - acc: 0.7921 - val_loss: 0.8850 - val_acc: 0.7840\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8439 - acc: 0.7929 - val_loss: 0.8930 - val_acc: 0.7800\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8444 - acc: 0.7929 - val_loss: 0.9048 - val_acc: 0.7790\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8419 - acc: 0.7923 - val_loss: 0.8976 - val_acc: 0.7880\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8455 - acc: 0.7924 - val_loss: 0.8898 - val_acc: 0.7770\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8428 - acc: 0.7915 - val_loss: 0.8907 - val_acc: 0.7920\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8436 - acc: 0.7915 - val_loss: 0.8913 - val_acc: 0.7770\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8439 - acc: 0.7909 - val_loss: 0.9018 - val_acc: 0.7780\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8446 - acc: 0.7937 - val_loss: 0.9372 - val_acc: 0.7620\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8465 - acc: 0.7924 - val_loss: 0.8923 - val_acc: 0.7780\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8416 - acc: 0.7941 - val_loss: 0.8832 - val_acc: 0.7840\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8400 - acc: 0.7947 - val_loss: 0.9604 - val_acc: 0.7550\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8469 - acc: 0.7945 - val_loss: 0.8965 - val_acc: 0.7820\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8403 - acc: 0.7952 - val_loss: 0.9046 - val_acc: 0.7760\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8402 - acc: 0.7947 - val_loss: 0.9169 - val_acc: 0.7670\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8412 - acc: 0.7933 - val_loss: 0.8864 - val_acc: 0.7830\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 0.8439 - acc: 0.7929 - val_loss: 0.8861 - val_acc: 0.7840\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8410 - acc: 0.7964 - val_loss: 0.8915 - val_acc: 0.7840\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8411 - acc: 0.7941 - val_loss: 0.8920 - val_acc: 0.7790\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8422 - acc: 0.7928 - val_loss: 0.8891 - val_acc: 0.7850\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8391 - acc: 0.7949 - val_loss: 0.8949 - val_acc: 0.7850\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8403 - acc: 0.7932 - val_loss: 0.8859 - val_acc: 0.7890\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 0.8423 - acc: 0.7931 - val_loss: 0.8853 - val_acc: 0.7830\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8368 - acc: 0.7971 - val_loss: 0.8855 - val_acc: 0.7900\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8394 - acc: 0.7953 - val_loss: 0.8868 - val_acc: 0.7920\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.8426 - acc: 0.7917 - val_loss: 0.8965 - val_acc: 0.7770\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8389 - acc: 0.7944 - val_loss: 0.8899 - val_acc: 0.7820\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8408 - acc: 0.7947 - val_loss: 0.8974 - val_acc: 0.7750\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 1s 69us/step - loss: 0.8405 - acc: 0.7943 - val_loss: 0.8912 - val_acc: 0.7820\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8369 - acc: 0.7971 - val_loss: 0.8802 - val_acc: 0.7880\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8399 - acc: 0.7937 - val_loss: 0.8901 - val_acc: 0.7810\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8384 - acc: 0.7953 - val_loss: 0.8948 - val_acc: 0.7760\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8414 - acc: 0.7932 - val_loss: 0.9244 - val_acc: 0.7680\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8411 - acc: 0.7972 - val_loss: 0.9254 - val_acc: 0.7690\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8358 - acc: 0.7955 - val_loss: 0.8906 - val_acc: 0.7870\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8387 - acc: 0.7917 - val_loss: 0.9011 - val_acc: 0.7790\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8370 - acc: 0.7935 - val_loss: 0.9040 - val_acc: 0.7770\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8379 - acc: 0.7960 - val_loss: 0.8839 - val_acc: 0.7830\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8369 - acc: 0.7968 - val_loss: 0.8946 - val_acc: 0.7790\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8393 - acc: 0.7961 - val_loss: 0.8885 - val_acc: 0.7800\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 0.8381 - acc: 0.7941 - val_loss: 0.8885 - val_acc: 0.7920\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8360 - acc: 0.7976 - val_loss: 0.8974 - val_acc: 0.7820\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8367 - acc: 0.7957 - val_loss: 0.9108 - val_acc: 0.7630\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 0.8368 - acc: 0.7964 - val_loss: 0.8884 - val_acc: 0.7910\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8347 - acc: 0.7959 - val_loss: 0.8868 - val_acc: 0.7860\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 0.8353 - acc: 0.7971 - val_loss: 0.9146 - val_acc: 0.7750\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8376 - acc: 0.7931 - val_loss: 0.8929 - val_acc: 0.7830\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8366 - acc: 0.7941 - val_loss: 0.8814 - val_acc: 0.7900\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8351 - acc: 0.7964 - val_loss: 0.8929 - val_acc: 0.7730\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8337 - acc: 0.7975 - val_loss: 0.9161 - val_acc: 0.7770\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8373 - acc: 0.7956 - val_loss: 0.9030 - val_acc: 0.7830\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8354 - acc: 0.7960 - val_loss: 0.8898 - val_acc: 0.7820\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8348 - acc: 0.7949 - val_loss: 0.9205 - val_acc: 0.7750\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8356 - acc: 0.7952 - val_loss: 0.8923 - val_acc: 0.7790\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8344 - acc: 0.7967 - val_loss: 0.8836 - val_acc: 0.7860\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 0.8367 - acc: 0.7975 - val_loss: 0.9089 - val_acc: 0.7750\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8375 - acc: 0.7957 - val_loss: 0.8977 - val_acc: 0.7750\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8360 - acc: 0.7952 - val_loss: 0.8912 - val_acc: 0.7900\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8348 - acc: 0.7963 - val_loss: 0.9026 - val_acc: 0.7770\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8337 - acc: 0.7944 - val_loss: 0.8806 - val_acc: 0.7900\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8383 - acc: 0.7949 - val_loss: 0.8953 - val_acc: 0.7790\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8333 - acc: 0.7956 - val_loss: 0.8870 - val_acc: 0.7950\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8326 - acc: 0.7971 - val_loss: 0.8948 - val_acc: 0.7890\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8348 - acc: 0.7964 - val_loss: 0.9087 - val_acc: 0.7770\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8312 - acc: 0.7984 - val_loss: 0.8846 - val_acc: 0.7910\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8338 - acc: 0.7975 - val_loss: 0.8993 - val_acc: 0.7830\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 0.8340 - acc: 0.7952 - val_loss: 0.8812 - val_acc: 0.7880\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 0.8339 - acc: 0.7981 - val_loss: 0.8903 - val_acc: 0.7860\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8329 - acc: 0.7996 - val_loss: 0.8872 - val_acc: 0.7880\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8315 - acc: 0.7964 - val_loss: 0.9010 - val_acc: 0.7790\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8321 - acc: 0.7971 - val_loss: 0.8818 - val_acc: 0.7910\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8360 - acc: 0.7936 - val_loss: 0.8848 - val_acc: 0.7940\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8320 - acc: 0.7964 - val_loss: 0.9189 - val_acc: 0.7730\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 0.8315 - acc: 0.7961 - val_loss: 0.8870 - val_acc: 0.7890\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.8356 - acc: 0.7964 - val_loss: 0.8949 - val_acc: 0.7900\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8296 - acc: 0.7963 - val_loss: 0.8993 - val_acc: 0.7740\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8294 - acc: 0.7985 - val_loss: 0.8843 - val_acc: 0.7940\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 54us/step - loss: 0.8371 - acc: 0.7977 - val_loss: 0.8813 - val_acc: 0.7860\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 0.8321 - acc: 0.7972 - val_loss: 0.8829 - val_acc: 0.7860\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 52us/step - loss: 0.8366 - acc: 0.7952 - val_loss: 0.8917 - val_acc: 0.7830\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 57us/step - loss: 0.8314 - acc: 0.7960 - val_loss: 0.8940 - val_acc: 0.7900\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 61us/step - loss: 0.8286 - acc: 0.7989 - val_loss: 0.8870 - val_acc: 0.7830\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 60us/step - loss: 0.8310 - acc: 0.7987 - val_loss: 0.8939 - val_acc: 0.7790\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8291 - acc: 0.7988 - val_loss: 0.9057 - val_acc: 0.7710\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8307 - acc: 0.7971 - val_loss: 0.9535 - val_acc: 0.7510\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8347 - acc: 0.7956 - val_loss: 0.8834 - val_acc: 0.7890\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8294 - acc: 0.7965 - val_loss: 0.9010 - val_acc: 0.7730\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 0.8306 - acc: 0.7965 - val_loss: 1.0312 - val_acc: 0.7230\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8363 - acc: 0.7961 - val_loss: 0.9395 - val_acc: 0.7610\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8322 - acc: 0.7969 - val_loss: 0.8920 - val_acc: 0.7890\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 0.8273 - acc: 0.7997 - val_loss: 0.8981 - val_acc: 0.7870\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 58us/step - loss: 0.8301 - acc: 0.7983 - val_loss: 0.8796 - val_acc: 0.7910\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:49:24.094678Z",
     "start_time": "2019-05-20T22:49:23.377521Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:49:34.376495Z",
     "start_time": "2019-05-20T22:49:33.726544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 62us/step\n",
      "1500/1500 [==============================] - 0s 114us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:49:35.272559Z",
     "start_time": "2019-05-20T22:49:35.262864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8208784712791443, 0.8012]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:49:40.622586Z",
     "start_time": "2019-05-20T22:49:40.614181Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9197876877784729, 0.7673333328564962]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:52:07.067504Z",
     "start_time": "2019-05-20T22:49:59.961941Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/michaelmoravetz/anaconda3/envs/learn-env/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 2s 257us/step - loss: 1.9581 - acc: 0.1501 - val_loss: 1.9271 - val_acc: 0.1620\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.9372 - acc: 0.1707 - val_loss: 1.9124 - val_acc: 0.1880\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.9269 - acc: 0.1856 - val_loss: 1.9012 - val_acc: 0.1970\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.9140 - acc: 0.2020 - val_loss: 1.8909 - val_acc: 0.2170\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.9028 - acc: 0.2075 - val_loss: 1.8797 - val_acc: 0.2220\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.8952 - acc: 0.2189 - val_loss: 1.8684 - val_acc: 0.2220\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 59us/step - loss: 1.8871 - acc: 0.2192 - val_loss: 1.8549 - val_acc: 0.2340\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.8698 - acc: 0.2420 - val_loss: 1.8413 - val_acc: 0.2470\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.8603 - acc: 0.2419 - val_loss: 1.8250 - val_acc: 0.2660\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.8435 - acc: 0.2535 - val_loss: 1.8066 - val_acc: 0.2730\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 65us/step - loss: 1.8325 - acc: 0.2651 - val_loss: 1.7884 - val_acc: 0.2910\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 1s 68us/step - loss: 1.8213 - acc: 0.2699 - val_loss: 1.7678 - val_acc: 0.3160\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 1s 67us/step - loss: 1.8012 - acc: 0.2921 - val_loss: 1.7453 - val_acc: 0.3340\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.7849 - acc: 0.2919 - val_loss: 1.7227 - val_acc: 0.3420\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.7769 - acc: 0.3025 - val_loss: 1.7004 - val_acc: 0.3600\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.7504 - acc: 0.3104 - val_loss: 1.6761 - val_acc: 0.3890\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.7323 - acc: 0.3244 - val_loss: 1.6499 - val_acc: 0.4050\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.7166 - acc: 0.3332 - val_loss: 1.6277 - val_acc: 0.4230\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.7005 - acc: 0.3340 - val_loss: 1.6032 - val_acc: 0.4330\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.6850 - acc: 0.3405 - val_loss: 1.5802 - val_acc: 0.4450\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.6697 - acc: 0.3505 - val_loss: 1.5577 - val_acc: 0.4470\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.6550 - acc: 0.3567 - val_loss: 1.5360 - val_acc: 0.4610\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.6369 - acc: 0.3688 - val_loss: 1.5155 - val_acc: 0.4700\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.6243 - acc: 0.3640 - val_loss: 1.4956 - val_acc: 0.4810\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 1.6012 - acc: 0.3837 - val_loss: 1.4720 - val_acc: 0.4970\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.5822 - acc: 0.3823 - val_loss: 1.4502 - val_acc: 0.5040\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.5705 - acc: 0.3908 - val_loss: 1.4313 - val_acc: 0.5250\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.5503 - acc: 0.4057 - val_loss: 1.4089 - val_acc: 0.5290\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 1s 119us/step - loss: 1.5400 - acc: 0.3989 - val_loss: 1.3913 - val_acc: 0.5420\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 1s 95us/step - loss: 1.5315 - acc: 0.4087 - val_loss: 1.3723 - val_acc: 0.5450\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.5184 - acc: 0.4117 - val_loss: 1.3552 - val_acc: 0.5500\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 1.4978 - acc: 0.4219 - val_loss: 1.3362 - val_acc: 0.5560\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.4810 - acc: 0.4299 - val_loss: 1.3186 - val_acc: 0.5690\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 1.4735 - acc: 0.4313 - val_loss: 1.3017 - val_acc: 0.5760\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 1.4577 - acc: 0.4392 - val_loss: 1.2846 - val_acc: 0.5910\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.4462 - acc: 0.4383 - val_loss: 1.2695 - val_acc: 0.5830\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 1.4325 - acc: 0.4436 - val_loss: 1.2506 - val_acc: 0.5980\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.4095 - acc: 0.4565 - val_loss: 1.2321 - val_acc: 0.6180\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.4098 - acc: 0.4511 - val_loss: 1.2186 - val_acc: 0.6250\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.3959 - acc: 0.4635 - val_loss: 1.2031 - val_acc: 0.6320\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.3850 - acc: 0.4660 - val_loss: 1.1888 - val_acc: 0.6340\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.3827 - acc: 0.4600 - val_loss: 1.1775 - val_acc: 0.6440\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 1.3600 - acc: 0.4735 - val_loss: 1.1631 - val_acc: 0.6480\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.3481 - acc: 0.4789 - val_loss: 1.1485 - val_acc: 0.6490\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 1.3476 - acc: 0.4817 - val_loss: 1.1371 - val_acc: 0.6540\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.3320 - acc: 0.4853 - val_loss: 1.1219 - val_acc: 0.6730\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.3259 - acc: 0.4897 - val_loss: 1.1151 - val_acc: 0.6580\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 1.3026 - acc: 0.5016 - val_loss: 1.0986 - val_acc: 0.6730\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.3000 - acc: 0.5020 - val_loss: 1.0870 - val_acc: 0.6730\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.2992 - acc: 0.5064 - val_loss: 1.0741 - val_acc: 0.6870\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.2839 - acc: 0.5036 - val_loss: 1.0609 - val_acc: 0.6910\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.2688 - acc: 0.5197 - val_loss: 1.0477 - val_acc: 0.6960\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.2480 - acc: 0.5213 - val_loss: 1.0369 - val_acc: 0.6930\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 1.2478 - acc: 0.5215 - val_loss: 1.0278 - val_acc: 0.6950\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 1.2413 - acc: 0.5259 - val_loss: 1.0173 - val_acc: 0.6980\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.2432 - acc: 0.5265 - val_loss: 1.0102 - val_acc: 0.6940\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 1.2249 - acc: 0.5381 - val_loss: 0.9987 - val_acc: 0.7030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 1.2248 - acc: 0.5349 - val_loss: 0.9910 - val_acc: 0.7060\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.2189 - acc: 0.5300 - val_loss: 0.9831 - val_acc: 0.7090\n",
      "Epoch 60/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.2119 - acc: 0.5320 - val_loss: 0.9738 - val_acc: 0.7050\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.1877 - acc: 0.5519 - val_loss: 0.9627 - val_acc: 0.7090\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.1915 - acc: 0.5429 - val_loss: 0.9584 - val_acc: 0.7150\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.1921 - acc: 0.5428 - val_loss: 0.9497 - val_acc: 0.7140\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.1785 - acc: 0.5559 - val_loss: 0.9408 - val_acc: 0.7210\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.1741 - acc: 0.5601 - val_loss: 0.9326 - val_acc: 0.7210\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 1.1669 - acc: 0.5532 - val_loss: 0.9253 - val_acc: 0.7210\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.1441 - acc: 0.5676 - val_loss: 0.9136 - val_acc: 0.7240\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.1366 - acc: 0.5684 - val_loss: 0.9058 - val_acc: 0.7310\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.1318 - acc: 0.5763 - val_loss: 0.8978 - val_acc: 0.7340\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.1248 - acc: 0.5737 - val_loss: 0.8900 - val_acc: 0.7260\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.1239 - acc: 0.5692 - val_loss: 0.8831 - val_acc: 0.7340\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.1225 - acc: 0.5697 - val_loss: 0.8788 - val_acc: 0.7410\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.1191 - acc: 0.5791 - val_loss: 0.8727 - val_acc: 0.7370\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 1.1120 - acc: 0.5815 - val_loss: 0.8662 - val_acc: 0.7370\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.0944 - acc: 0.5840 - val_loss: 0.8605 - val_acc: 0.7410\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.0911 - acc: 0.5892 - val_loss: 0.8534 - val_acc: 0.7410\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 1.0972 - acc: 0.5803 - val_loss: 0.8497 - val_acc: 0.7450\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.0884 - acc: 0.5887 - val_loss: 0.8413 - val_acc: 0.7500\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.0817 - acc: 0.5933 - val_loss: 0.8370 - val_acc: 0.7530\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.0622 - acc: 0.6045 - val_loss: 0.8285 - val_acc: 0.7510\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 1s 71us/step - loss: 1.0671 - acc: 0.6009 - val_loss: 0.8220 - val_acc: 0.7500\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.0669 - acc: 0.5968 - val_loss: 0.8170 - val_acc: 0.7560\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.0560 - acc: 0.6067 - val_loss: 0.8134 - val_acc: 0.7500\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.0587 - acc: 0.6031 - val_loss: 0.8066 - val_acc: 0.7580\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 1.0438 - acc: 0.6037 - val_loss: 0.8035 - val_acc: 0.7570\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 1.0360 - acc: 0.6091 - val_loss: 0.7976 - val_acc: 0.7580\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.0317 - acc: 0.6217 - val_loss: 0.7921 - val_acc: 0.7600\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 1.0319 - acc: 0.6008 - val_loss: 0.7847 - val_acc: 0.7590\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0144 - acc: 0.6223 - val_loss: 0.7834 - val_acc: 0.7630\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.0193 - acc: 0.6259 - val_loss: 0.7772 - val_acc: 0.7630\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 1.0138 - acc: 0.6193 - val_loss: 0.7732 - val_acc: 0.7620\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 1.0116 - acc: 0.6297 - val_loss: 0.7680 - val_acc: 0.7580\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.9917 - acc: 0.6311 - val_loss: 0.7613 - val_acc: 0.7620\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.9929 - acc: 0.6345 - val_loss: 0.7595 - val_acc: 0.7650\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 1.0000 - acc: 0.6216 - val_loss: 0.7559 - val_acc: 0.7660\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.9858 - acc: 0.6312 - val_loss: 0.7508 - val_acc: 0.7640\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.9774 - acc: 0.6396 - val_loss: 0.7459 - val_acc: 0.7650\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.9705 - acc: 0.6329 - val_loss: 0.7410 - val_acc: 0.7680\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 1.0011 - acc: 0.6269 - val_loss: 0.7395 - val_acc: 0.7650\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9846 - acc: 0.6320 - val_loss: 0.7391 - val_acc: 0.7660\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.9731 - acc: 0.6404 - val_loss: 0.7341 - val_acc: 0.7700\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.9731 - acc: 0.6337 - val_loss: 0.7295 - val_acc: 0.7720\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.9643 - acc: 0.6405 - val_loss: 0.7267 - val_acc: 0.7710\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.9554 - acc: 0.6467 - val_loss: 0.7237 - val_acc: 0.7680\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9594 - acc: 0.6403 - val_loss: 0.7194 - val_acc: 0.7770\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.9633 - acc: 0.6379 - val_loss: 0.7211 - val_acc: 0.7690\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.9369 - acc: 0.6525 - val_loss: 0.7131 - val_acc: 0.7730\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.9394 - acc: 0.6481 - val_loss: 0.7114 - val_acc: 0.7690\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.9393 - acc: 0.6503 - val_loss: 0.7065 - val_acc: 0.7760\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.9358 - acc: 0.6460 - val_loss: 0.7037 - val_acc: 0.7760\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.9327 - acc: 0.6525 - val_loss: 0.7063 - val_acc: 0.7700\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9315 - acc: 0.6489 - val_loss: 0.7004 - val_acc: 0.7700\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.9149 - acc: 0.6616 - val_loss: 0.6964 - val_acc: 0.7710\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.9258 - acc: 0.6548 - val_loss: 0.6948 - val_acc: 0.7740\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.9213 - acc: 0.6501 - val_loss: 0.6909 - val_acc: 0.7710\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.9056 - acc: 0.6641 - val_loss: 0.6884 - val_acc: 0.7740\n",
      "Epoch 117/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.9244 - acc: 0.6560 - val_loss: 0.6878 - val_acc: 0.7710\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.9114 - acc: 0.6601 - val_loss: 0.6827 - val_acc: 0.7720\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.9095 - acc: 0.6664 - val_loss: 0.6814 - val_acc: 0.7730\n",
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.9094 - acc: 0.6613 - val_loss: 0.6816 - val_acc: 0.7720\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.9179 - acc: 0.6573 - val_loss: 0.6756 - val_acc: 0.7770\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8910 - acc: 0.6651 - val_loss: 0.6751 - val_acc: 0.7740\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 1s 73us/step - loss: 0.9065 - acc: 0.6635 - val_loss: 0.6725 - val_acc: 0.7770\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.8874 - acc: 0.6721 - val_loss: 0.6721 - val_acc: 0.7760\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8829 - acc: 0.6723 - val_loss: 0.6659 - val_acc: 0.7790\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 1s 79us/step - loss: 0.9000 - acc: 0.6652 - val_loss: 0.6679 - val_acc: 0.7770\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8751 - acc: 0.6732 - val_loss: 0.6658 - val_acc: 0.7770\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.8787 - acc: 0.6775 - val_loss: 0.6635 - val_acc: 0.7760\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8780 - acc: 0.6665 - val_loss: 0.6614 - val_acc: 0.7760\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 0.8732 - acc: 0.6737 - val_loss: 0.6602 - val_acc: 0.7750\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 1s 72us/step - loss: 0.8657 - acc: 0.6711 - val_loss: 0.6546 - val_acc: 0.7810\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.8622 - acc: 0.6739 - val_loss: 0.6533 - val_acc: 0.7740\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8772 - acc: 0.6749 - val_loss: 0.6572 - val_acc: 0.7740\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 1s 70us/step - loss: 0.8686 - acc: 0.6801 - val_loss: 0.6512 - val_acc: 0.7810\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8620 - acc: 0.6831 - val_loss: 0.6492 - val_acc: 0.7810\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 1s 78us/step - loss: 0.8656 - acc: 0.6772 - val_loss: 0.6460 - val_acc: 0.7790\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 0.8535 - acc: 0.6813 - val_loss: 0.6497 - val_acc: 0.7730\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8492 - acc: 0.6828 - val_loss: 0.6442 - val_acc: 0.7790\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.8517 - acc: 0.6845 - val_loss: 0.6409 - val_acc: 0.7820\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8570 - acc: 0.6801 - val_loss: 0.6403 - val_acc: 0.7850\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 0.8617 - acc: 0.6759 - val_loss: 0.6409 - val_acc: 0.7790\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8622 - acc: 0.6807 - val_loss: 0.6391 - val_acc: 0.7840\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8508 - acc: 0.6888 - val_loss: 0.6366 - val_acc: 0.7800\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.8314 - acc: 0.6931 - val_loss: 0.6368 - val_acc: 0.7850\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.8219 - acc: 0.6911 - val_loss: 0.6339 - val_acc: 0.7830\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 1s 96us/step - loss: 0.8346 - acc: 0.6901 - val_loss: 0.6314 - val_acc: 0.7840\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8347 - acc: 0.6876 - val_loss: 0.6312 - val_acc: 0.7840\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 1s 87us/step - loss: 0.8420 - acc: 0.6913 - val_loss: 0.6325 - val_acc: 0.7800\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8370 - acc: 0.6927 - val_loss: 0.6276 - val_acc: 0.7810\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.8206 - acc: 0.6972 - val_loss: 0.6244 - val_acc: 0.7830\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 1s 94us/step - loss: 0.8230 - acc: 0.6943 - val_loss: 0.6266 - val_acc: 0.7800\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 0.8170 - acc: 0.6953 - val_loss: 0.6243 - val_acc: 0.7850\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.8061 - acc: 0.6939 - val_loss: 0.6200 - val_acc: 0.7870\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 1s 93us/step - loss: 0.8064 - acc: 0.6987 - val_loss: 0.6199 - val_acc: 0.7850\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 1s 92us/step - loss: 0.8225 - acc: 0.7048 - val_loss: 0.6179 - val_acc: 0.7840\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.8053 - acc: 0.7015 - val_loss: 0.6164 - val_acc: 0.7870\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.8124 - acc: 0.6944 - val_loss: 0.6173 - val_acc: 0.7840\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.8215 - acc: 0.6901 - val_loss: 0.6179 - val_acc: 0.7830\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.8081 - acc: 0.6999 - val_loss: 0.6140 - val_acc: 0.7880\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8014 - acc: 0.7080 - val_loss: 0.6125 - val_acc: 0.7860\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.8096 - acc: 0.7055 - val_loss: 0.6141 - val_acc: 0.7870\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 1s 114us/step - loss: 0.8053 - acc: 0.7012 - val_loss: 0.6147 - val_acc: 0.7840\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 1s 99us/step - loss: 0.8129 - acc: 0.6980 - val_loss: 0.6127 - val_acc: 0.7880\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 1s 98us/step - loss: 0.7874 - acc: 0.7027 - val_loss: 0.6096 - val_acc: 0.7860\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.7852 - acc: 0.7065 - val_loss: 0.6086 - val_acc: 0.7820\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.7896 - acc: 0.7096 - val_loss: 0.6041 - val_acc: 0.7890\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.7898 - acc: 0.7093 - val_loss: 0.6048 - val_acc: 0.7820\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 1s 85us/step - loss: 0.7908 - acc: 0.7031 - val_loss: 0.6045 - val_acc: 0.7900\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7870 - acc: 0.7073 - val_loss: 0.6030 - val_acc: 0.7890\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.7976 - acc: 0.7068 - val_loss: 0.6037 - val_acc: 0.7840\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.7756 - acc: 0.7080 - val_loss: 0.6026 - val_acc: 0.7850\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 1s 88us/step - loss: 0.7869 - acc: 0.7123 - val_loss: 0.5989 - val_acc: 0.7850\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.8004 - acc: 0.6985 - val_loss: 0.6018 - val_acc: 0.7870\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 1s 75us/step - loss: 0.7872 - acc: 0.7063 - val_loss: 0.6034 - val_acc: 0.7810\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.7806 - acc: 0.7084 - val_loss: 0.5997 - val_acc: 0.7860\n",
      "Epoch 176/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.7764 - acc: 0.7096 - val_loss: 0.5974 - val_acc: 0.7880\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7780 - acc: 0.7093 - val_loss: 0.6001 - val_acc: 0.7810\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7864 - acc: 0.7041 - val_loss: 0.5969 - val_acc: 0.7870\n",
      "Epoch 179/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7706 - acc: 0.7141 - val_loss: 0.5960 - val_acc: 0.7830\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.7699 - acc: 0.7149 - val_loss: 0.5950 - val_acc: 0.7890\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 1s 90us/step - loss: 0.7634 - acc: 0.7153 - val_loss: 0.5960 - val_acc: 0.7850\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.7683 - acc: 0.7145 - val_loss: 0.5913 - val_acc: 0.7880\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.7621 - acc: 0.7184 - val_loss: 0.5947 - val_acc: 0.7850\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.7696 - acc: 0.7140 - val_loss: 0.5917 - val_acc: 0.7890\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7688 - acc: 0.7153 - val_loss: 0.5916 - val_acc: 0.7900\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 1s 77us/step - loss: 0.7611 - acc: 0.7159 - val_loss: 0.5904 - val_acc: 0.7790\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 1s 91us/step - loss: 0.7546 - acc: 0.7193 - val_loss: 0.5890 - val_acc: 0.7840\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.7503 - acc: 0.7213 - val_loss: 0.5911 - val_acc: 0.7780\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.7413 - acc: 0.7248 - val_loss: 0.5875 - val_acc: 0.7800\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.7537 - acc: 0.7223 - val_loss: 0.5864 - val_acc: 0.7870\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.7478 - acc: 0.7263 - val_loss: 0.5850 - val_acc: 0.7820\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 1s 83us/step - loss: 0.7481 - acc: 0.7244 - val_loss: 0.5873 - val_acc: 0.7780\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 1s 80us/step - loss: 0.7533 - acc: 0.7137 - val_loss: 0.5864 - val_acc: 0.7850\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 1s 82us/step - loss: 0.7541 - acc: 0.7208 - val_loss: 0.5855 - val_acc: 0.7840\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 1s 84us/step - loss: 0.7523 - acc: 0.7243 - val_loss: 0.5853 - val_acc: 0.7790\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 1s 76us/step - loss: 0.7415 - acc: 0.7289 - val_loss: 0.5827 - val_acc: 0.7830\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 1s 86us/step - loss: 0.7374 - acc: 0.7281 - val_loss: 0.5844 - val_acc: 0.7820\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.7251 - acc: 0.7305 - val_loss: 0.5823 - val_acc: 0.7820\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 1s 81us/step - loss: 0.7417 - acc: 0.7209 - val_loss: 0.5824 - val_acc: 0.7790\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 1s 89us/step - loss: 0.7279 - acc: 0.7217 - val_loss: 0.5802 - val_acc: 0.7800\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:52:15.795194Z",
     "start_time": "2019-05-20T22:52:15.148243Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 1s 67us/step\n",
      "1500/1500 [==============================] - 0s 86us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:52:19.672498Z",
     "start_time": "2019-05-20T22:52:19.663166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45614987654685973, 0.8506666666984558]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:52:31.213361Z",
     "start_time": "2019-05-20T22:52:31.206067Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6101703035036723, 0.7840000004768372]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:53:57.205896Z",
     "start_time": "2019-05-20T22:52:49.519640Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:57:53.965077Z",
     "start_time": "2019-05-20T22:54:05.728938Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 3s 89us/step - loss: 1.9356 - acc: 0.1736 - val_loss: 1.9004 - val_acc: 0.2333\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 1.8514 - acc: 0.2924 - val_loss: 1.7932 - val_acc: 0.3373\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 1.6979 - acc: 0.4034 - val_loss: 1.6066 - val_acc: 0.4533\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 1.4886 - acc: 0.5093 - val_loss: 1.3941 - val_acc: 0.5493\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 1.2835 - acc: 0.5927 - val_loss: 1.2056 - val_acc: 0.6160\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 1.1154 - acc: 0.6442 - val_loss: 1.0573 - val_acc: 0.6597\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.9895 - acc: 0.6746 - val_loss: 0.9516 - val_acc: 0.6790\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.8984 - acc: 0.6975 - val_loss: 0.8753 - val_acc: 0.6980\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.8316 - acc: 0.7138 - val_loss: 0.8200 - val_acc: 0.7110\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.7819 - acc: 0.7238 - val_loss: 0.7791 - val_acc: 0.7183\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.7433 - acc: 0.7348 - val_loss: 0.7448 - val_acc: 0.7283\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.7129 - acc: 0.7443 - val_loss: 0.7204 - val_acc: 0.7363\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.6877 - acc: 0.7505 - val_loss: 0.6993 - val_acc: 0.7387\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.6671 - acc: 0.7574 - val_loss: 0.6818 - val_acc: 0.7477\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.6491 - acc: 0.7640 - val_loss: 0.6694 - val_acc: 0.7537\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.6333 - acc: 0.7684 - val_loss: 0.6575 - val_acc: 0.7563\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.6194 - acc: 0.7738 - val_loss: 0.6489 - val_acc: 0.7553\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 2s 61us/step - loss: 0.6071 - acc: 0.7784 - val_loss: 0.6385 - val_acc: 0.7627\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.5955 - acc: 0.7823 - val_loss: 0.6315 - val_acc: 0.7633\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 2s 70us/step - loss: 0.5856 - acc: 0.7853 - val_loss: 0.6255 - val_acc: 0.7687\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.5760 - acc: 0.7890 - val_loss: 0.6163 - val_acc: 0.7707\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 2s 64us/step - loss: 0.5670 - acc: 0.7925 - val_loss: 0.6118 - val_acc: 0.7703\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 2s 69us/step - loss: 0.5590 - acc: 0.7958 - val_loss: 0.6078 - val_acc: 0.7700\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.5510 - acc: 0.7990 - val_loss: 0.6043 - val_acc: 0.7747\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.5437 - acc: 0.8022 - val_loss: 0.5967 - val_acc: 0.7780\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.5366 - acc: 0.8056 - val_loss: 0.5935 - val_acc: 0.7800\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.5300 - acc: 0.8072 - val_loss: 0.5897 - val_acc: 0.7793\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.5238 - acc: 0.8100 - val_loss: 0.5869 - val_acc: 0.7797\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.5180 - acc: 0.8128 - val_loss: 0.5832 - val_acc: 0.7850\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 2s 61us/step - loss: 0.5125 - acc: 0.8157 - val_loss: 0.5801 - val_acc: 0.7847\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.5069 - acc: 0.8169 - val_loss: 0.5787 - val_acc: 0.7833\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.5015 - acc: 0.8196 - val_loss: 0.5752 - val_acc: 0.7853\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.4968 - acc: 0.8205 - val_loss: 0.5734 - val_acc: 0.7847\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.4919 - acc: 0.8232 - val_loss: 0.5714 - val_acc: 0.7887\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.4867 - acc: 0.8255 - val_loss: 0.5680 - val_acc: 0.7893\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.4825 - acc: 0.8266 - val_loss: 0.5675 - val_acc: 0.7873\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.4782 - acc: 0.8289 - val_loss: 0.5662 - val_acc: 0.7880\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.4739 - acc: 0.8296 - val_loss: 0.5646 - val_acc: 0.7880\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.4698 - acc: 0.8321 - val_loss: 0.5622 - val_acc: 0.7907\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 2s 62us/step - loss: 0.4660 - acc: 0.8338 - val_loss: 0.5611 - val_acc: 0.7890\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.4622 - acc: 0.8344 - val_loss: 0.5591 - val_acc: 0.7927\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 2s 74us/step - loss: 0.4584 - acc: 0.8362 - val_loss: 0.5587 - val_acc: 0.7900\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.4546 - acc: 0.8379 - val_loss: 0.5567 - val_acc: 0.7947\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.4511 - acc: 0.8395 - val_loss: 0.5585 - val_acc: 0.7923\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.4481 - acc: 0.8404 - val_loss: 0.5560 - val_acc: 0.7963\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.4447 - acc: 0.8417 - val_loss: 0.5553 - val_acc: 0.7933\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.4413 - acc: 0.8431 - val_loss: 0.5544 - val_acc: 0.7957\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.4381 - acc: 0.8437 - val_loss: 0.5557 - val_acc: 0.7940\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.4351 - acc: 0.8461 - val_loss: 0.5556 - val_acc: 0.7943\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.4319 - acc: 0.8467 - val_loss: 0.5528 - val_acc: 0.7970\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.4293 - acc: 0.8481 - val_loss: 0.5521 - val_acc: 0.7983\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.4263 - acc: 0.8483 - val_loss: 0.5515 - val_acc: 0.7980\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.4238 - acc: 0.8504 - val_loss: 0.5512 - val_acc: 0.7997\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.4204 - acc: 0.8519 - val_loss: 0.5539 - val_acc: 0.7967\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.4179 - acc: 0.8519 - val_loss: 0.5539 - val_acc: 0.7957\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 2s 61us/step - loss: 0.4155 - acc: 0.8532 - val_loss: 0.5514 - val_acc: 0.8003\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 2s 66us/step - loss: 0.4132 - acc: 0.8531 - val_loss: 0.5543 - val_acc: 0.7993\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 3s 79us/step - loss: 0.4105 - acc: 0.8548 - val_loss: 0.5515 - val_acc: 0.8017\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 62us/step - loss: 0.4081 - acc: 0.8553 - val_loss: 0.5523 - val_acc: 0.8017\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 2s 70us/step - loss: 0.4057 - acc: 0.8570 - val_loss: 0.5521 - val_acc: 0.8003\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 2s 70us/step - loss: 0.4032 - acc: 0.8578 - val_loss: 0.5548 - val_acc: 0.7983\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.4012 - acc: 0.8584 - val_loss: 0.5523 - val_acc: 0.8013\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.3987 - acc: 0.8605 - val_loss: 0.5559 - val_acc: 0.7977\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 2s 66us/step - loss: 0.3967 - acc: 0.8604 - val_loss: 0.5546 - val_acc: 0.8013\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 2s 65us/step - loss: 0.3945 - acc: 0.8619 - val_loss: 0.5543 - val_acc: 0.7997\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3922 - acc: 0.8617 - val_loss: 0.5540 - val_acc: 0.7993\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3903 - acc: 0.8616 - val_loss: 0.5575 - val_acc: 0.7987\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3881 - acc: 0.8638 - val_loss: 0.5541 - val_acc: 0.8023\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3863 - acc: 0.8634 - val_loss: 0.5560 - val_acc: 0.7987\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 2s 61us/step - loss: 0.3840 - acc: 0.8650 - val_loss: 0.5553 - val_acc: 0.7990\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3822 - acc: 0.8659 - val_loss: 0.5596 - val_acc: 0.7980\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3800 - acc: 0.8674 - val_loss: 0.5571 - val_acc: 0.7997\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3787 - acc: 0.8667 - val_loss: 0.5580 - val_acc: 0.8010\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3764 - acc: 0.8680 - val_loss: 0.5579 - val_acc: 0.7963\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3747 - acc: 0.8682 - val_loss: 0.5616 - val_acc: 0.7967\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3727 - acc: 0.8691 - val_loss: 0.5615 - val_acc: 0.7993\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3713 - acc: 0.8700 - val_loss: 0.5618 - val_acc: 0.7987\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3691 - acc: 0.8706 - val_loss: 0.5597 - val_acc: 0.7997\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3678 - acc: 0.8708 - val_loss: 0.5629 - val_acc: 0.7967\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3657 - acc: 0.8719 - val_loss: 0.5624 - val_acc: 0.7963\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3645 - acc: 0.8725 - val_loss: 0.5643 - val_acc: 0.7963\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3624 - acc: 0.8732 - val_loss: 0.5636 - val_acc: 0.7983\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 2s 57us/step - loss: 0.3610 - acc: 0.8754 - val_loss: 0.5653 - val_acc: 0.7993\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.3593 - acc: 0.8754 - val_loss: 0.5681 - val_acc: 0.7980\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3580 - acc: 0.8752 - val_loss: 0.5639 - val_acc: 0.8000\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.3559 - acc: 0.8765 - val_loss: 0.5673 - val_acc: 0.7967\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.3544 - acc: 0.8775 - val_loss: 0.5687 - val_acc: 0.7953\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.3530 - acc: 0.8774 - val_loss: 0.5677 - val_acc: 0.7980\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3516 - acc: 0.8778 - val_loss: 0.5700 - val_acc: 0.7977\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 2s 48us/step - loss: 0.3498 - acc: 0.8794 - val_loss: 0.5701 - val_acc: 0.7983\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3485 - acc: 0.8791 - val_loss: 0.5708 - val_acc: 0.7973\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 2s 50us/step - loss: 0.3472 - acc: 0.8791 - val_loss: 0.5760 - val_acc: 0.7960\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 2s 51us/step - loss: 0.3456 - acc: 0.8800 - val_loss: 0.5712 - val_acc: 0.7967\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 2s 47us/step - loss: 0.3439 - acc: 0.8804 - val_loss: 0.5748 - val_acc: 0.7963\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 2s 46us/step - loss: 0.3429 - acc: 0.8811 - val_loss: 0.5753 - val_acc: 0.7967\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 2s 49us/step - loss: 0.3412 - acc: 0.8816 - val_loss: 0.5744 - val_acc: 0.7990\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3400 - acc: 0.8826 - val_loss: 0.5761 - val_acc: 0.7950\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3389 - acc: 0.8823 - val_loss: 0.5776 - val_acc: 0.7977\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3373 - acc: 0.8825 - val_loss: 0.5779 - val_acc: 0.7973\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3359 - acc: 0.8838 - val_loss: 0.5772 - val_acc: 0.7987\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 2s 63us/step - loss: 0.3344 - acc: 0.8845 - val_loss: 0.5804 - val_acc: 0.7977\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 2s 75us/step - loss: 0.3333 - acc: 0.8844 - val_loss: 0.5805 - val_acc: 0.7983\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3320 - acc: 0.8849 - val_loss: 0.5798 - val_acc: 0.7970\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.3305 - acc: 0.8861 - val_loss: 0.5887 - val_acc: 0.7960\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.3295 - acc: 0.8862 - val_loss: 0.5838 - val_acc: 0.7963\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 2s 60us/step - loss: 0.3282 - acc: 0.8867 - val_loss: 0.5857 - val_acc: 0.7960\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.3268 - acc: 0.8874 - val_loss: 0.5862 - val_acc: 0.7977\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 2s 56us/step - loss: 0.3258 - acc: 0.8875 - val_loss: 0.5861 - val_acc: 0.7983\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3248 - acc: 0.8880 - val_loss: 0.5879 - val_acc: 0.7957\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3234 - acc: 0.8887 - val_loss: 0.5881 - val_acc: 0.7973\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3217 - acc: 0.8885 - val_loss: 0.5905 - val_acc: 0.7957\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.3210 - acc: 0.8888 - val_loss: 0.5893 - val_acc: 0.7973\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 2s 52us/step - loss: 0.3195 - acc: 0.8894 - val_loss: 0.5915 - val_acc: 0.7970\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 2s 59us/step - loss: 0.3183 - acc: 0.8900 - val_loss: 0.5955 - val_acc: 0.7947\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 2s 58us/step - loss: 0.3174 - acc: 0.8901 - val_loss: 0.5935 - val_acc: 0.7953\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 2s 55us/step - loss: 0.3162 - acc: 0.8909 - val_loss: 0.5971 - val_acc: 0.7963\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 2s 53us/step - loss: 0.3150 - acc: 0.8906 - val_loss: 0.5953 - val_acc: 0.7990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.3140 - acc: 0.8912 - val_loss: 0.5990 - val_acc: 0.7937\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.3130 - acc: 0.8919 - val_loss: 0.5991 - val_acc: 0.7947\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 2s 54us/step - loss: 0.3115 - acc: 0.8922 - val_loss: 0.6003 - val_acc: 0.7987\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:58:04.076492Z",
     "start_time": "2019-05-20T22:58:01.550215Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 2s 67us/step\n",
      "4000/4000 [==============================] - 0s 72us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:58:13.796014Z",
     "start_time": "2019-05-20T22:58:13.778312Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3090777450727694, 0.8932121212121212]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T22:58:23.659473Z",
     "start_time": "2019-05-20T22:58:23.645034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.59181216776371, 0.80425]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
